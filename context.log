RESP:


semantic_search, {"query":"paper fine-tuning fine tuning how does this paper do fine-tuning"}
TOOL CALL RESULT:

Here are the top-10 text chunks that matches the query sorted in descending order.
```
```

CONTEXT:
RESP:


semantic_search, {"query":"fine-tuning"}
TOOL CALL RESULT:

Here are the top-10 text chunks that matches the query sorted in descending order.
```
```

CONTEXT:
RESP:
Which paper do you mean? Provide the paper PDF, title, or the repo/file path (or paste the fine-tuning section) and I’ll extract how it fine-tunes.

CONTEXT:
RESP:


semantic_search, {"query":"fine-tuning paper fine-tuning how does this paper do fine-tuning"}
TOOL CALL RESULT:

Here are the top-10 text chunks that matches the query sorted in descending order.
```
Sequence: 226
Headings: Llama 2: Open Foundation and Fine-Tuned Chat Models,4 Safety,4.2 Safety Fine-Tuning
<content>
In this section, we describe our approach to safety fine-tuning, including safety categories, annotation
guidelines, and the techniques we use to mitigate safety risks. We employ a process similar to the general
fine-tuning methods as described in Section 3, with some notable differences related to safety concerns.
Specifically, we use the following techniques in safety fine-tuning:</content>
Sequence: 76
Headings: Llama 2: Open Foundation and Fine-Tuned Chat Models,3 Fine-tuning,3.1 Supervised Fine-Tuning (SFT)
<content>
**Fine-Tuning Details.** For supervised fine-tuning, we use a cosine learning rate schedule with an initial
learning rate of 2 _×_ 10 _[−]_ [5], a weight decay of 0.1, a batch size of 64, and a sequence length of 4096 tokens.</content>
Sequence: 20
Headings: Llama 2: Open Foundation and Fine-Tuned Chat Models,1 Introduction
<content>
Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data
annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,
this paper contributes a thorough description of our fine-tuning methodology and approach to improving
LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and</content>
Sequence: 64
Headings: Llama 2: Open Foundation and Fine-Tuned Chat Models,3 Fine-tuning
<content>
In this section, we report on our experiments and findings using supervised fine-tuning (Section 3.1), as
well as initial and iterative reward modeling (Section 3.2.2) and RLHF (Section 3.2.3). We also share a
new technique, Ghost Attention (GAtt), which we find helps control dialogue flow over multiple turns
(Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.


8</content>
Sequence: 514
Headings: Llama 2: Open Foundation and Fine-Tuned Chat Models,A Appendix,A.3 Additional Details for Fine-tuning
<content>
GSM8k and MATH tasks.


**A.3** **Additional Details for Fine-tuning**</content>
Sequence: 317
Headings: Llama 2: Open Foundation and Fine-Tuned Chat Models,5 Discussion,5.1 Learnings and Observations
<content>
Even with proficient annotators, each individual writes with significant variation. A model fine-tuned on
SFT annotation learns this diversity, including, unfortunately, the tail-end of poorly executed annotation. Furthermore, the model’s performance is capped by the writing abilities of the most skilled annotators. Human
annotators are arguably less subject to discrepancy when comparing two outputs’ preference annotation</content>
Sequence: 10
Headings: Llama 2: Open Foundation and Fine-Tuned Chat Models,Contents
<content>
A.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46


A.2 Additional Details for Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47


A.3 Additional Details for Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51


A.4 Additional Details for Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58</content>
Sequence: 227
Headings: Llama 2: Open Foundation and Fine-Tuned Chat Models,4 Safety,4.2 Safety Fine-Tuning
<content>
1. **Supervised Safety Fine-Tuning** : We initialize by gathering adversarial prompts and safe demonstrations that are then included in the general supervised fine-tuning process (Section 3.1). This teaches
the model to align with our safety guidelines even before RLHF, and thus lays the foundation for
high-quality human preference data annotation.</content>
Sequence: 357
Headings: Llama 2: Open Foundation and Fine-Tuned Chat Models,6 Related Work
<content>
Ouyang et al. (2022) demonstrates that a combination of instruction fine-tuning and RLHF can help fix
issues with factuality, toxicity, and helpfulness that cannot be remedied by simply scaling up LLMs. Bai
et al. (2022b) partially automates this fine-tuning-plus-RLHF approach by replacing the human-labeled
fine-tuning data with the model’s own self-critiques and revisions, and by replacing human raters with a</content>
Sequence: 234
Headings: Llama 2: Open Foundation and Fine-Tuned Chat Models,4 Safety,4.2 Safety Fine-Tuning,4.2.2 Safety Supervised Fine-Tuning
<content>
In accordance with the established guidelines from Section 4.2.1, we gather prompts and demonstrations
of safe model responses from trained annotators, and use the data for supervised fine-tuning in the same
manner as described in Section 3.1. An example can be found in Table 5.</content>
```

CONTEXT:
RESP:
They do supervised fine-tuning (SFT) on instruction + safety data (cosine LR schedule, init LR 2e‑5, weight decay 0.1, batch size 64, seq length 4096), train reward models from human preference annotations, then apply iterative RLHF (PPO) — with safety SFT using adversarial prompts/safe demonstrations and a Ghost Attention (GAtt) trick to control multi‑turn dialogue.

CONTEXT:
