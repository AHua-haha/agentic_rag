{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_text_splitters import ExperimentalMarkdownSyntaxTextSplitter\n",
    "from langchain_text_splitters import  CharacterTextSplitter\n",
    "\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"h1\"),\n",
    "    (\"##\", \"h2\"),\n",
    "    (\"###\", \"h3\"),\n",
    "    (\"####\", \"h4\"),\n",
    "]\n",
    "markdown_splitter = ExperimentalMarkdownSyntaxTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    ")\n",
    "with open('MinerU_2307.09288v2__20251127030211.md', 'r') as file:\n",
    "    content = file.read()\n",
    "    md_header_splits = markdown_splitter.split_text(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_header_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for doc in md_header_splits:\n",
    "    metadata = doc.metadata\n",
    "    headings = []\n",
    "    if \"h1\" in metadata:\n",
    "        headings.append(metadata[\"h1\"])\n",
    "    if \"h2\" in metadata:\n",
    "        headings.append(metadata[\"h2\"])\n",
    "    if \"h3\" in metadata:\n",
    "        headings.append(metadata[\"h3\"])\n",
    "    if \"h4\" in metadata:\n",
    "        headings.append(metadata[\"h4\"])\n",
    "\n",
    "    res.append({\n",
    "        \"headings\": headings,\n",
    "        \"content\": doc.page_content\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "id_count = 1\n",
    "\n",
    "heading_tree = dict()\n",
    "for doc in md_header_splits:\n",
    "    metadata = doc.metadata\n",
    "    headings = []\n",
    "    if \"h1\" in metadata:\n",
    "        headings.append(metadata[\"h1\"])\n",
    "    if \"h2\" in metadata:\n",
    "        headings.append(metadata[\"h2\"])\n",
    "    if \"h3\" in metadata:\n",
    "        headings.append(metadata[\"h3\"])\n",
    "    if \"h4\" in metadata:\n",
    "        headings.append(metadata[\"h4\"])\n",
    "    length = len(headings)\n",
    "    def formatHeader(i: int) -> str :\n",
    "        header = '#' * (i + 1)\n",
    "        return f\"{header} {headings[i]}\"\n",
    "    obj = heading_tree.setdefault(formatHeader(length - 1), {\"child\": []})\n",
    "    obj[\"id\"] = id_count\n",
    "    id_count += 1\n",
    "    obj[\"headings\"] = headings\n",
    "    obj[\"chunks\"] = re.split(r'\\n{2,}', doc.page_content.strip())\n",
    "    for i in range(length - 1):\n",
    "        parent = formatHeader(i)\n",
    "        child = formatHeader(i + 1)\n",
    "        obj = heading_tree.setdefault(parent, {\"child\": []})\n",
    "        if child not in obj[\"child\"] :\n",
    "            obj[\"child\"].append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(heading_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"doc.json\", 'w') as json_file:\n",
    "    json.dump(res, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# LLAMA 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron* Louis Martin† Kevin Stone†  \n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialom*\n",
      "## Abstract\n",
      "## Contents\n",
      "## 1 Introduction\n",
      "## 2 Pretraining\n",
      "## 3 Fine-tuning\n",
      "## 4 Safety\n",
      "## 5 Discussion\n",
      "## 6 Related Work\n",
      "## 7 Conclusion\n",
      "## References\n",
      "## A Appendix\n",
      "# LLAMA 2: Open Foundation and Fine-Tuned Chat Models\n",
      "## Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called  $\\text{LLAMA} \\, 2-\\text{CHAT}$ , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-CHAT in order to enable the community to build on our work and contribute to the responsible development of LLMs.\n",
      "# LLAMA 2: Open Foundation and Fine-Tuned Chat Models\n",
      "## Contents\n",
      "1 Introduction 3\n",
      "2 Pretraining 5  \n",
      "2.1 Pretraining Data 5\n",
      "2.2 Training Details 5\n",
      "2.3 LLAMA 2 Pretrained Model Evaluation 7  \n",
      "3 Fine-tuning 8  \n",
      "3.1 Supervised Fine-Tuning (SFT) 9\n",
      "3.2 Reinforcement Learning with Human Feedback (RLHF) 9\n",
      "3.3 System Message for Multi-Turn Consistency 16\n",
      "3.4 RLHF Results 17  \n",
      "4 Safety 20  \n",
      "4.1 Safety in Pretraining 20\n",
      "4.2 Safety Fine-Tuning 23\n",
      "4.3 Red Teaming 28\n",
      "4.4 Safety Evaluation of LLAMA 2-CHAT 29  \n",
      "5 Discussion 32  \n",
      "5.1 Learnings and Observations 32\n",
      "5.2 Limitations and Ethical Considerations 34\n",
      "5.3 Responsible Release Strategy 35  \n",
      "6 Related Work 35\n",
      "7 Conclusion 36\n",
      "A Appendix 46  \n",
      "A.1 Contributions 46\n",
      "A.2 Additional Details for Pretraining 47\n",
      "A.3 Additional Details for Fine-tuning 51\n",
      "A.4 Additional Details for Safety 58\n",
      "A.5 Data Annotation 72\n",
      "A.6 Dataset Contamination 75\n",
      "A.7 Model Card 77  \n",
      "![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/efb40aa2874ba2af90fd13e297e03de151aa27d82aa3a701dc9cb341dc7f1f3b.jpg)\n",
      "Figure 1: Helpfulness human evaluation results for LLAMA 2-CHAT compared to other open-source and closed-source models. Human raters compared model generations on  $\\sim 4k$  prompts consisting of both single and multi-turn prompts. The  $95\\%$  confidence intervals for this evaluation are between  $1\\%$  and  $2\\%$ . More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent difficulty of comparing generations.  \n",
      "![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/08f6863778b4206402cec247ce6410b9fcb0e70bd55573ad296b7ec92db20a46.jpg)\n",
      "Figure 2: Win-rate % for helpfulness and safety between commercial-licensed baselines and LAMA 2-CHAT, according to GPT-4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.\n",
      "# LLAMA 2: Open Foundation and Fine-Tuned Chat Models\n",
      "## 1 Introduction\n",
      "Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.  \n",
      "The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022), but none of these models are suitable substitutes for closed \"product\" LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.  \n",
      "In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs,  $LLAMA$  2 and  $LLAMA$  2-CHAT, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,  $LLAMA$  2-CHAT models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of  $LLAMA$  2 and  $LLAMA$  2-CHAT, such as the emergence of tool usage and temporal organization of knowledge.  \n",
      "![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/c8fd7ef2dcd67577f41264332a402eb3c7eb9ea4614d0af1a0144a2bc2c511b6.jpg)\n",
      "Figure 3: Safety human evaluation results for LLM2-CHAT compared to other open-source and closed-source models. Human raters judged model generations for safety violations across  $\\sim 2,000$  adversarial prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the LLM2-CHAT models.  \n",
      "We are releasing the following models to the general public for research and commercial use:  \n",
      "1. LLAMA 2, an updated version of LlAMA 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by  $40\\%$ , doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of LlAMA 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing. $\\S$\n",
      "2. LLMA 2-CHAT, a fine-tuned version of LLMA 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well.  \n",
      "We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, LAMA 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover all scenarios. Therefore, before deploying any applications of LAMA 2-CHAT, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guide<sup>®</sup> and code examples<sup>®</sup> to facilitate the safe deployment of LAMA 2 and LAMA 2-CHAT. More details of our responsible release strategy can be found in Section 5.3.  \n",
      "The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology (Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related work (Section 6), and conclusions (Section 7).  \n",
      "![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/3e2b7379f53cb7a2d0215aa5335b7b488fcc8b1bb979e5997227df401065e2a0.jpg)\n",
      "Figure 4: Training of LlAMA 2-CHAT: This process begins with the pretraining of LlAMA 2 using publicly available online sources. Following this, we create an initial version of LlAMA 2-CHAT through the application of supervised fine-tuning. Subsequently, the model is iteratively refined using Reinforcement Learning with Human Feedback (RLHF) methodologies, specifically through rejection sampling and Proximal Policy Optimization (PPO). Throughout the RLHF stage, the accumulation of iterative reward modeling data in parallel with model enhancements is crucial to ensure the reward models remain within distribution.\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    doc = md_header_splits[i]\n",
    "    h = doc.metadata[\"headings\"]\n",
    "    for line in h:\n",
    "        print(line)\n",
    "    print(doc.page_content)\n",
    "    value = heading_tree.get(h[-1], {\"child\": []})\n",
    "    for line in value[\"child\"]:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Identify the key concepts mentioned in the following document. For each key concept, \n",
    "summarize how it is introduced, explained, and discussed throughout the document. Be sure to highlight any examples, case studies, or specific arguments related to each concept.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"anthropic/claude-haiku-4.5\",  # Specify a model available on OpenRouter\n",
    "    temperature=0.1,\n",
    "    api_key=\"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\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = \"\"\"\n",
    "You are an expert document summarizer. Your task is to summarize and refine document.\n",
    "\n",
    "You MUST:\n",
    "- Identify the main concepts of the document. \n",
    "- Briefly describe the main concepts, what it is.\n",
    "- Summarize what the content introduce and discuss about the main concept.\n",
    "- Summarize what each sub section introduce and discuss about the main concept.\n",
    "- keep the summary simple, short and concise.\n",
    "\n",
    "IMPORTANT: you MUST summarizze the document solely on the document contents.\n",
    "IMPORTANT: DO NOT use markdown headings, output two paragraph, first identify and describe the main concept, second summarize how the main concept is discessed.\n",
    "IMPORTANT: refer each sub section in this format: sub-section ** <name> **.\n",
    "\n",
    "Response examples:\n",
    "<example>\n",
    "This section discuss about the main concept vector database, it is a database searching by vector, mainly used for semantic search.\n",
    "\n",
    "sub-section ** feature ** discuss about the feature about the vector database.\n",
    "sub-section ** usage ** discuss about the how to use vector database.\n",
    "</example>\n",
    "\n",
    "The contents under each sub section is omitted.\n",
    "\n",
    "Summarize this document.\n",
    "\n",
    "<DOCUMENT>\n",
    "{}\n",
    "</DOCUMENT>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_prompt = \"\"\"\n",
    "You are an expert document chunk summarizer. Your task is to summarize and refine each chunk in document.\n",
    "The document contents is separate to several chunks.\n",
    "\n",
    "You MUST:\n",
    "- Identify the core concepts of each chunk, figure out what each chunk is talk about.\n",
    "- Briefly describe the core concepts, what it is.\n",
    "- For each chunk, summarize and refine what this chunk introduce and discuss about the core concept.\n",
    "- keep the summary simple, short and concise.\n",
    "\n",
    "IMPORTANT: for each chunk summary, do not use markdown headings, output two paragraph, first describe the core concepts, second describe how the core concept is discussed in this chunk.\n",
    "IMPORTANT: refer each sub section in this format: sub-section ** <name> **.\n",
    "IMPORTANT: for each chunk summary, do not involve contents out of this chunk.\n",
    "\n",
    "Summarize each chunk in this document.\n",
    "\n",
    "<DOCUMENT>\n",
    "{}\n",
    "</DOCUMENT>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc = \"\"\"\n",
    "# LLAMA 2: Open Foundation and Fine-Tuned Chat Models\n",
    "Hugo Touvron* Louis Martin† Kevin Stone†  \n",
    "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
    "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
    "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
    "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
    "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
    "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
    "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
    "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
    "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
    "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
    "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
    "Sergey Edunov Thomas Scialom*\n",
    "## Abstract\n",
    "## Contents\n",
    "## 1 Introduction\n",
    "## 2 Pretraining\n",
    "## 3 Fine-tuning\n",
    "## 4 Safety\n",
    "## 5 Discussion\n",
    "## 6 Related Work\n",
    "## 7 Conclusion\n",
    "## References\n",
    "## A Appendix\n",
    "\"\"\"\n",
    "\n",
    "msg = summary_prompt.format(doc)\n",
    "resp = model.invoke(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunk_doc = \"\"\"\n",
    "\n",
    "Section structure:\n",
    "```\n",
    "# LLAMA 2: Open Foundation and Fine-Tuned Chat Models\n",
    "\n",
    "<chunk contents>\n",
    "\n",
    "## Abstract\n",
    "## Contents\n",
    "## 1 Introduction\n",
    "## 2 Pretraining\n",
    "## 3 Fine-tuning\n",
    "## 4 Safety\n",
    "## 5 Discussion\n",
    "## 6 Related Work\n",
    "## 7 Conclusion\n",
    "## References\n",
    "## A Appendix\n",
    "```\n",
    "\n",
    "Chunk contents:\n",
    "```\n",
    "# LLAMA 2: Open Foundation and Fine-Tuned Chat Models\n",
    "\n",
    "<chunk>\n",
    "<id>1</id>\n",
    "Hugo Touvron* Louis Martin† Kevin Stone†  \n",
    "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
    "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
    "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
    "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
    "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
    "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
    "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
    "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
    "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
    "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
    "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
    "Sergey Edunov Thomas Scialom*\n",
    "</chunk>\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "msg = chunk_prompt.format(chunk_doc)\n",
    "resp = model.invoke(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Chunk 1 Summary\n",
      "\n",
      "The core concept of this chunk is the **authorship and attribution** of the LLAMA 2 research paper. This section identifies the large collaborative team of researchers responsible for developing and publishing the LLAMA 2 open foundation and fine-tuned chat models.\n",
      "\n",
      "This chunk presents an extensive list of author names affiliated with the LLAMA 2 project, establishing the collaborative effort behind the research. The authors are listed with their names and institutional affiliations (indicated by symbols like * and †), representing a significant research team involved in creating the foundation models and chat variants discussed in the paper. This authorship section serves as the formal attribution for the work presented throughout the document.\n"
     ]
    }
   ],
   "source": [
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ProviderStrategy\n",
    "\n",
    "class summary(BaseModel):\n",
    "    id: int\n",
    "    summary: str\n",
    "\n",
    "class Response(BaseModel):\n",
    "    chunk_summary: list[summary]\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=chunk_prompt.format(chunk_doc),\n",
    "    response_format=ProviderStrategy(Response),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "StructuredOutputValidationError",
     "evalue": "Failed to parse structured output for tool 'Response': Native structured output expected valid JSON for Response, but parsing failed: Expecting value: line 1 column 1 (char 0)..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/workspace/agentic_rag/.venv/lib/python3.10/site-packages/langchain/agents/structured_output.py:380\u001b[0m, in \u001b[0;36mProviderStrategyBinding.parse\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/workspace/agentic_rag/.venv/lib/python3.10/site-packages/langchain/agents/factory.py:888\u001b[0m, in \u001b[0;36mcreate_agent.<locals>._handle_model_output\u001b[0;34m(output, effective_response_format)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     structured_response \u001b[38;5;241m=\u001b[39m \u001b[43mprovider_strategy_binding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# noqa: BLE001\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/agentic_rag/.venv/lib/python3.10/site-packages/langchain/agents/structured_output.py:387\u001b[0m, in \u001b[0;36mProviderStrategyBinding.parse\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    383\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNative structured output expected valid JSON for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut parsing failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m     )\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m# Parse according to schema\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Native structured output expected valid JSON for Response, but parsing failed: Expecting value: line 1 column 1 (char 0).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mStructuredOutputValidationError\u001b[0m           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/agentic_rag/.venv/lib/python3.10/site-packages/langgraph/pregel/main.py:3068\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3065\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3066\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3068\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   3069\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3070\u001b[0m     config,\n\u001b[1;32m   3071\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   3072\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3073\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[1;32m   3075\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[1;32m   3076\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   3077\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   3078\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   3079\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[1;32m   3080\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3081\u001b[0m ):\n\u001b[1;32m   3082\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3083\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/workspace/agentic_rag/.venv/lib/python3.10/site-packages/langgraph/pregel/main.py:2643\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2641\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2642\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2643\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2644\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2645\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2646\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2647\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2648\u001b[0m ):\n\u001b[1;32m   2649\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2650\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2651\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2652\u001b[0m     )\n\u001b[1;32m   2653\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m~/workspace/agentic_rag/.venv/lib/python3.10/site-packages/langgraph/pregel/_runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/workspace/agentic_rag/.venv/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/workspace/agentic_rag/.venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:656\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 656\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/workspace/agentic_rag/.venv/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:400\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 400\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/workspace/agentic_rag/.venv/lib/python3.10/site-packages/langchain/agents/factory.py:1129\u001b[0m, in \u001b[0;36mcreate_agent.<locals>.model_node\u001b[0;34m(state, runtime)\u001b[0m\n\u001b[1;32m   1116\u001b[0m request \u001b[38;5;241m=\u001b[39m ModelRequest(\n\u001b[1;32m   1117\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1118\u001b[0m     tools\u001b[38;5;241m=\u001b[39mdefault_tools,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1124\u001b[0m     runtime\u001b[38;5;241m=\u001b[39mruntime,\n\u001b[1;32m   1125\u001b[0m )\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# No handlers - execute directly\u001b[39;00m\n\u001b[0;32m-> 1129\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_model_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m# Call composed handler with base handler\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     response \u001b[38;5;241m=\u001b[39m wrap_model_call_handler(request, _execute_model_sync)\n",
      "File \u001b[0;32m~/workspace/agentic_rag/.venv/lib/python3.10/site-packages/langchain/agents/factory.py:1105\u001b[0m, in \u001b[0;36mcreate_agent.<locals>._execute_model_sync\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   1102\u001b[0m output \u001b[38;5;241m=\u001b[39m model_\u001b[38;5;241m.\u001b[39minvoke(messages)\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;66;03m# Handle model output to get messages and structured_response\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m handled_output \u001b[38;5;241m=\u001b[39m \u001b[43m_handle_model_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_response_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1106\u001b[0m messages_list \u001b[38;5;241m=\u001b[39m handled_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1107\u001b[0m structured_response \u001b[38;5;241m=\u001b[39m handled_output\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstructured_response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/agentic_rag/.venv/lib/python3.10/site-packages/langchain/agents/factory.py:894\u001b[0m, in \u001b[0;36mcreate_agent.<locals>._handle_model_output\u001b[0;34m(output, effective_response_format)\u001b[0m\n\u001b[1;32m    890\u001b[0m     schema_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m    891\u001b[0m         effective_response_format\u001b[38;5;241m.\u001b[39mschema_spec\u001b[38;5;241m.\u001b[39mschema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    892\u001b[0m     )\n\u001b[1;32m    893\u001b[0m     validation_error \u001b[38;5;241m=\u001b[39m StructuredOutputValidationError(schema_name, exc, output)\n\u001b[0;32m--> 894\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [output], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstructured_response\u001b[39m\u001b[38;5;124m\"\u001b[39m: structured_response}\n",
      "\u001b[0;31mStructuredOutputValidationError\u001b[0m: Failed to parse structured output for tool 'Response': Native structured output expected valid JSON for Response, but parsing failed: Expecting value: line 1 column 1 (char 0).."
     ]
    }
   ],
   "source": [
    "response = agent.invoke(\n",
    "    input={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
