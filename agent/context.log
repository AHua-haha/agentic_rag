User:

The document shows the partial table of contents and the remaining contents of the document.
Read the table of content and the remaining contents thoroughly, conplete the table of contents of the document. Record it with the tool.

IMPORTANT: understand the section hierarchy and make sure the level of section is correct.
IMPORTANT: generate the table of content solely based on the document contents.
IMPORTANT: this only shows part of the document, NEVER guess the table of contents if you are not sure.
IMPORTANT: if you can not identify the end of some section, record the end of the section with -1, change it when you can identify the section end.

<DOCUMENT>
[0]: # LLAMA 2: Open Foundation and Fine-Tuned Chat Models
[1]: Hugo Touvron* Louis Martin† Kevin Stone†
[2]: Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra  
Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen  
Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller  
Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou  
Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev  
Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich  
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra  
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi  
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang  
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang  
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic  
Sergey Edunov Thomas Scialom*
[3]: ## GenAI,Meta
[4]: ## Abstract
[5]: In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called  $\text{LLAMA} \, 2-\text{CHAT}$ , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-CHAT in order to enable the community to build on our work and contribute to the responsible development of LLMs.
[6]: ## Contents
[7]: 1 Introduction 3  
2 Pretraining 5
[8]: 2.1 Pretraining Data 5  
2.2 Training Details 5  
2.3 LLAMA 2 Pretrained Model Evaluation 7
[9]: 3 Fine-tuning 8
[10]: 3.1 Supervised Fine-Tuning (SFT) 9  
3.2 Reinforcement Learning with Human Feedback (RLHF) 9  
3.3 System Message for Multi-Turn Consistency 16  
3.4 RLHF Results 17
[11]: 4 Safety 20
[12]: 4.1 Safety in Pretraining 20  
4.2 Safety Fine-Tuning 23  
4.3 Red Teaming 28  
4.4 Safety Evaluation of LLAMA 2-CHAT 29
[13]: 5 Discussion 32
[14]: 5.1 Learnings and Observations 32  
5.2 Limitations and Ethical Considerations 34  
5.3 Responsible Release Strategy 35
[15]: 6 Related Work 35  
7 Conclusion 36  
A Appendix 46
[16]: A.1 Contributions 46  
A.2 Additional Details for Pretraining 47  
A.3 Additional Details for Fine-tuning 51  
A.4 Additional Details for Safety 58  
A.5 Data Annotation 72  
A.6 Dataset Contamination 75  
A.7 Model Card 77
[17]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/efb40aa2874ba2af90fd13e297e03de151aa27d82aa3a701dc9cb341dc7f1f3b.jpg)  
Figure 1: Helpfulness human evaluation results for LLAMA 2-CHAT compared to other open-source and closed-source models. Human raters compared model generations on  $\sim 4k$  prompts consisting of both single and multi-turn prompts. The  $95\%$  confidence intervals for this evaluation are between  $1\%$  and  $2\%$ . More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent difficulty of comparing generations.
[18]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/08f6863778b4206402cec247ce6410b9fcb0e70bd55573ad296b7ec92db20a46.jpg)  
Figure 2: Win-rate % for helpfulness and safety between commercial-licensed baselines and LAMA 2-CHAT, according to GPT-4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.
[19]: ## 1 Introduction
</DOCUMENT>

RESP:


doc_toc, {"toc": [{"level":0,"name":"Abstract","start":4,"end":5},{"level":0,"name":"Contents","start":6,"end":16},{"level":0,"name":"1 Introduction","start":19,"end":-1},{"level":0,"name":"2 Pretraining","start":7,"end":-1},{"level":1,"name":"2.1 Pretraining Data","start":8,"end":-1},{"level":1,"name":"2.2 Training Details","start":8,"end":-1},{"level":1,"name":"2.3 LLAMA 2 Pretrained Model Evaluation","start":8,"end":-1},{"level":0,"name":"3 Fine-tuning","start":9,"end":-1},{"level":1,"name":"3.1 Supervised Fine-Tuning (SFT)","start":10,"end":-1},{"level":1,"name":"3.2 Reinforcement Learning with Human Feedback (RLHF)","start":10,"end":-1},{"level":1,"name":"3.3 System Message for Multi-Turn Consistency","start":10,"end":-1},{"level":1,"name":"3.4 RLHF Results","start":10,"end":-1},{"level":0,"name":"4 Safety","start":11,"end":-1},{"level":1,"name":"4.1 Safety in Pretraining","start":12,"end":-1},{"level":1,"name":"4.2 Safety Fine-Tuning","start":12,"end":-1},{"level":1,"name":"4.3 Red Teaming","start":12,"end":-1},{"level":1,"name":"4.4 Safety Evaluation of LLAMA 2-CHAT","start":12,"end":-1},{"level":0,"name":"5 Discussion","start":13,"end":-1},{"level":1,"name":"5.1 Learnings and Observations","start":14,"end":-1},{"level":1,"name":"5.2 Limitations and Ethical Considerations","start":14,"end":-1},{"level":1,"name":"5.3 Responsible Release Strategy","start":14,"end":-1},{"level":0,"name":"6 Related Work","start":15,"end":-1},{"level":0,"name":"7 Conclusion","start":15,"end":-1},{"level":0,"name":"A Appendix","start":15,"end":-1},{"level":1,"name":"A.1 Contributions","start":16,"end":-1},{"level":1,"name":"A.2 Additional Details for Pretraining","start":16,"end":-1},{"level":1,"name":"A.3 Additional Details for Fine-tuning","start":16,"end":-1},{"level":1,"name":"A.4 Additional Details for Safety","start":16,"end":-1},{"level":1,"name":"A.5 Data Annotation","start":16,"end":-1},{"level":1,"name":"A.6 Dataset Contamination","start":16,"end":-1},{"level":1,"name":"A.7 Model Card","start":16,"end":-1}]}
TOOL CALL RESULT:
record document toc success
CONTEXT:
RESP:


CONTEXT:
User:

The document shows the partial table of contents and the remaining contents of the document.
Read the table of content and the remaining contents thoroughly, conplete the table of contents of the document. Record it with the tool.

IMPORTANT: understand the section hierarchy and make sure the level of section is correct.
IMPORTANT: generate the table of content solely based on the document contents.
IMPORTANT: this only shows part of the document, NEVER guess the table of contents if you are not sure.
IMPORTANT: if you can not identify the end of some section, record the end of the section with -1, change it when you can identify the section end.

<DOCUMENT>
 Abstract (start: 4, end: 5)
 Contents (start: 6, end: 16)
 1 Introduction (start: 19, end: -1)
 2 Pretraining (start: 7, end: -1)
   2.1 Pretraining Data (start: 8, end: -1)
   2.2 Training Details (start: 8, end: -1)
   2.3 LLAMA 2 Pretrained Model Evaluation (start: 8, end: -1)
 3 Fine-tuning (start: 9, end: -1)
   3.1 Supervised Fine-Tuning (SFT) (start: 10, end: -1)
   3.2 Reinforcement Learning with Human Feedback (RLHF) (start: 10, end: -1)
   3.3 System Message for Multi-Turn Consistency (start: 10, end: -1)
   3.4 RLHF Results (start: 10, end: -1)
 4 Safety (start: 11, end: -1)
   4.1 Safety in Pretraining (start: 12, end: -1)
   4.2 Safety Fine-Tuning (start: 12, end: -1)
   4.3 Red Teaming (start: 12, end: -1)
   4.4 Safety Evaluation of LLAMA 2-CHAT (start: 12, end: -1)
 5 Discussion (start: 13, end: -1)
   5.1 Learnings and Observations (start: 14, end: -1)
   5.2 Limitations and Ethical Considerations (start: 14, end: -1)
   5.3 Responsible Release Strategy (start: 14, end: -1)
 6 Related Work (start: 15, end: -1)
 7 Conclusion (start: 15, end: -1)
 A Appendix (start: 15, end: -1)
   A.1 Contributions (start: 16, end: -1)
   A.2 Additional Details for Pretraining (start: 16, end: -1)
   A.3 Additional Details for Fine-tuning (start: 16, end: -1)
   A.4 Additional Details for Safety (start: 16, end: -1)
   A.5 Data Annotation (start: 16, end: -1)
   A.6 Dataset Contamination (start: 16, end: -1)
   A.7 Model Card (start: 16, end: -1)
[17]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/efb40aa2874ba2af90fd13e297e03de151aa27d82aa3a701dc9cb341dc7f1f3b.jpg)  
Figure 1: Helpfulness human evaluation results for LLAMA 2-CHAT compared to other open-source and closed-source models. Human raters compared model generations on  $\sim 4k$  prompts consisting of both single and multi-turn prompts. The  $95\%$  confidence intervals for this evaluation are between  $1\%$  and  $2\%$ . More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent difficulty of comparing generations.
[18]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/08f6863778b4206402cec247ce6410b9fcb0e70bd55573ad296b7ec92db20a46.jpg)  
Figure 2: Win-rate % for helpfulness and safety between commercial-licensed baselines and LAMA 2-CHAT, according to GPT-4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.
[19]: ## 1 Introduction
[20]: Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.
[21]: The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022), but none of these models are suitable substitutes for closed "product" LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.
[22]: In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs,  $LLAMA$  2 and  $LLAMA$  2-CHAT, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,  $LLAMA$  2-CHAT models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of  $LLAMA$  2 and  $LLAMA$  2-CHAT, such as the emergence of tool usage and temporal organization of knowledge.
[23]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/c8fd7ef2dcd67577f41264332a402eb3c7eb9ea4614d0af1a0144a2bc2c511b6.jpg)  
Figure 3: Safety human evaluation results for LLM2-CHAT compared to other open-source and closed-source models. Human raters judged model generations for safety violations across  $\sim 2,000$  adversarial prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the LLM2-CHAT models.
[24]: We are releasing the following models to the general public for research and commercial use:
[25]: 1. LLAMA 2, an updated version of LlAMA 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by  $40\%$ , doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of LlAMA 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing. $\S$  
2. LLMA 2-CHAT, a fine-tuned version of LLMA 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well.
[26]: We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, LAMA 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover all scenarios. Therefore, before deploying any applications of LAMA 2-CHAT, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guide<sup>®</sup> and code examples<sup>®</sup> to facilitate the safe deployment of LAMA 2 and LAMA 2-CHAT. More details of our responsible release strategy can be found in Section 5.3.
[27]: The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology (Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related work (Section 6), and conclusions (Section 7).
[28]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/3e2b7379f53cb7a2d0215aa5335b7b488fcc8b1bb979e5997227df401065e2a0.jpg)  
Figure 4: Training of LlAMA 2-CHAT: This process begins with the pretraining of LlAMA 2 using publicly available online sources. Following this, we create an initial version of LlAMA 2-CHAT through the application of supervised fine-tuning. Subsequently, the model is iteratively refined using Reinforcement Learning with Human Feedback (RLHF) methodologies, specifically through rejection sampling and Proximal Policy Optimization (PPO). Throughout the RLHF stage, the accumulation of iterative reward modeling data in parallel with model enhancements is crucial to ensure the reward models remain within distribution.
[29]: ## 2 Pretraining
[30]: To create the new family of LlAMA 2 models, we began with the pretraining approach described in Touvron et al. (2023), using an optimized auto-regressive transformer, but made several changes to improve performance. Specifically, we performed more robust data cleaning, updated our data mixes, trained on  $40\%$  more total tokens, doubled the context length, and used grouped-query attention (GQA) to improve inference scalability for our larger models. Table 1 compares the attributes of the new LlAMA 2 models with the LlAMA 1 models.
[31]: ### 2.1 Pretraining Data
[32]: Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta's products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance-cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.
[33]: We performed a variety of pretraining data investigations so that users can better understand the potential capabilities and limitations of our models; results can be found in Section 4.1.
[34]: ### 2.2 Training Details
[35]: We adopt most of the pretraining setting and model architecture from  $\text{LLAMA 1}$ . We use the standard transformer architecture (Vaswani et al., 2017), apply pre-normalization using RMSNorm (Zhang and Sennrich, 2019), use the SwiGLU activation function (Shazeer, 2020), and rotary positional embeddings (RoPE, Su et al. 2022). The primary architectural differences from  $\text{LLAMA 1}$  include increased context length and grouped-query attention (GQA). We detail in Appendix Section A.2.1 each of these differences with ablation experiments to demonstrate their importance.
[36]: Hyperparameters. We trained using the AdamW optimizer (Loshchilov and Hutter, 2017), with  $\beta_{1} = 0.9$ ,  $\beta_{2} = 0.95$ ,  $\mathrm{eps} = 10^{-5}$ . We use a cosine learning rate schedule, with warmup of 2000 steps, and decay final learning rate down to  $10\%$  of the peak learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. Figure 5 (a) shows the training loss for LAMA 2 with these hyperparameters.
</DOCUMENT>

RESP:


doc_toc, {"toc": [{"level":0,"name":"Abstract","start":4,"end":5},{"level":0,"name":"Contents","start":6,"end":16},{"level":0,"name":"1 Introduction","start":19,"end":28},{"level":0,"name":"2 Pretraining","start":29,"end":-1},{"level":1,"name":"2.1 Pretraining Data","start":31,"end":33},{"level":1,"name":"2.2 Training Details","start":34,"end":-1},{"level":1,"name":"2.3 LLAMA 2 Pretrained Model Evaluation","start":-1,"end":-1},{"level":0,"name":"3 Fine-tuning","start":-1,"end":-1},{"level":1,"name":"3.1 Supervised Fine-Tuning (SFT)","start":-1,"end":-1},{"level":1,"name":"3.2 Reinforcement Learning with Human Feedback (RLHF)","start":-1,"end":-1},{"level":1,"name":"3.3 System Message for Multi-Turn Consistency","start":-1,"end":-1},{"level":1,"name":"3.4 RLHF Results","start":-1,"end":-1},{"level":0,"name":"4 Safety","start":-1,"end":-1},{"level":1,"name":"4.1 Safety in Pretraining","start":-1,"end":-1},{"level":1,"name":"4.2 Safety Fine-Tuning","start":-1,"end":-1},{"level":1,"name":"4.3 Red Teaming","start":-1,"end":-1},{"level":1,"name":"4.4 Safety Evaluation of LLAMA 2-CHAT","start":-1,"end":-1},{"level":0,"name":"5 Discussion","start":-1,"end":-1},{"level":1,"name":"5.1 Learnings and Observations","start":-1,"end":-1},{"level":1,"name":"5.2 Limitations and Ethical Considerations","start":-1,"end":-1},{"level":1,"name":"5.3 Responsible Release Strategy","start":-1,"end":-1},{"level":0,"name":"6 Related Work","start":-1,"end":-1},{"level":0,"name":"7 Conclusion","start":-1,"end":-1},{"level":0,"name":"A Appendix","start":-1,"end":-1},{"level":1,"name":"A.1 Contributions","start":-1,"end":-1},{"level":1,"name":"A.2 Additional Details for Pretraining","start":-1,"end":-1},{"level":1,"name":"A.3 Additional Details for Fine-tuning","start":-1,"end":-1},{"level":1,"name":"A.4 Additional Details for Safety","start":-1,"end":-1},{"level":1,"name":"A.5 Data Annotation","start":-1,"end":-1},{"level":1,"name":"A.6 Dataset Contamination","start":-1,"end":-1},{"level":1,"name":"A.7 Model Card","start":-1,"end":-1}]}
TOOL CALL RESULT:
record document toc success
CONTEXT:
RESP:


CONTEXT:
User:

The document shows the partial table of contents and the remaining contents of the document.
Read the table of content and the remaining contents thoroughly, conplete the table of contents of the document. Record it with the tool.

IMPORTANT: understand the section hierarchy and make sure the level of section is correct.
IMPORTANT: generate the table of content solely based on the document contents.
IMPORTANT: this only shows part of the document, NEVER guess the table of contents if you are not sure.
IMPORTANT: if you can not identify the end of some section, record the end of the section with -1, change it when you can identify the section end.

<DOCUMENT>
 Abstract (start: 4, end: 5)
 Contents (start: 6, end: 16)
 1 Introduction (start: 19, end: 28)
 2 Pretraining (start: 29, end: -1)
   2.1 Pretraining Data (start: 31, end: 33)
   2.2 Training Details (start: 34, end: -1)
   2.3 LLAMA 2 Pretrained Model Evaluation (start: -1, end: -1)
 3 Fine-tuning (start: -1, end: -1)
   3.1 Supervised Fine-Tuning (SFT) (start: -1, end: -1)
   3.2 Reinforcement Learning with Human Feedback (RLHF) (start: -1, end: -1)
   3.3 System Message for Multi-Turn Consistency (start: -1, end: -1)
   3.4 RLHF Results (start: -1, end: -1)
 4 Safety (start: -1, end: -1)
   4.1 Safety in Pretraining (start: -1, end: -1)
   4.2 Safety Fine-Tuning (start: -1, end: -1)
   4.3 Red Teaming (start: -1, end: -1)
   4.4 Safety Evaluation of LLAMA 2-CHAT (start: -1, end: -1)
 5 Discussion (start: -1, end: -1)
   5.1 Learnings and Observations (start: -1, end: -1)
   5.2 Limitations and Ethical Considerations (start: -1, end: -1)
   5.3 Responsible Release Strategy (start: -1, end: -1)
 6 Related Work (start: -1, end: -1)
 7 Conclusion (start: -1, end: -1)
 A Appendix (start: -1, end: -1)
   A.1 Contributions (start: -1, end: -1)
   A.2 Additional Details for Pretraining (start: -1, end: -1)
   A.3 Additional Details for Fine-tuning (start: -1, end: -1)
   A.4 Additional Details for Safety (start: -1, end: -1)
   A.5 Data Annotation (start: -1, end: -1)
   A.6 Dataset Contamination (start: -1, end: -1)
   A.7 Model Card (start: -1, end: -1)
[34]: ### 2.2 Training Details
[35]: We adopt most of the pretraining setting and model architecture from  $\text{LLAMA 1}$ . We use the standard transformer architecture (Vaswani et al., 2017), apply pre-normalization using RMSNorm (Zhang and Sennrich, 2019), use the SwiGLU activation function (Shazeer, 2020), and rotary positional embeddings (RoPE, Su et al. 2022). The primary architectural differences from  $\text{LLAMA 1}$  include increased context length and grouped-query attention (GQA). We detail in Appendix Section A.2.1 each of these differences with ablation experiments to demonstrate their importance.
[36]: Hyperparameters. We trained using the AdamW optimizer (Loshchilov and Hutter, 2017), with  $\beta_{1} = 0.9$ ,  $\beta_{2} = 0.95$ ,  $\mathrm{eps} = 10^{-5}$ . We use a cosine learning rate schedule, with warmup of 2000 steps, and decay final learning rate down to  $10\%$  of the peak learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. Figure 5 (a) shows the training loss for LAMA 2 with these hyperparameters.
[37]: <table><tr><td></td><td>Training Data</td><td>Params</td><td>Context Length</td><td>GQA</td><td>Tokens</td><td>LR</td></tr><tr><td rowspan="4">LLAMA 1</td><td rowspan="4">See Touvron et al. (2023)</td><td>7B</td><td>2k</td><td>×</td><td>1.0T</td><td>3.0 × 10-4</td></tr><tr><td>13B</td><td>2k</td><td>×</td><td>1.0T</td><td>3.0 × 10-4</td></tr><tr><td>33B</td><td>2k</td><td>×</td><td>1.4T</td><td>1.5 × 10-4</td></tr><tr><td>65B</td><td>2k</td><td>×</td><td>1.4T</td><td>1.5 × 10-4</td></tr><tr><td rowspan="4">LLAMA 2</td><td rowspan="4">A new mix of publicly available online data</td><td>7B</td><td>4k</td><td>×</td><td>2.0T</td><td>3.0 × 10-4</td></tr><tr><td>13B</td><td>4k</td><td>×</td><td>2.0T</td><td>3.0 × 10-4</td></tr><tr><td>34B</td><td>4k</td><td>✓</td><td>2.0T</td><td>1.5 × 10-4</td></tr><tr><td>70B</td><td>4k</td><td>✓</td><td>2.0T</td><td>1.5 × 10-4</td></tr></table>
[38]: Table 1: LlAMA 2 family of models. Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models — 34B and 70B — use Grouped-Query Attention (GQA) for improved inference scalability.
[39]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/29a21b4a91d2682caa52a8c0bc4be1f403400f9c1ea590ae5bcd7f7894a6e0ff.jpg)  
Figure 5: Training Loss for LlAMA 2 models. We compare the training loss of the LlAMA 2 family of models. We observe that after pretraining on 2T Tokens, the models still did not show any sign of saturation.
[40]: Tokenizer. We use the same tokenizer as  $\text{LLAMA 1}$ ; it employs a bptein encoding (BPE) algorithm (Sennrich et al., 2016) using the implementation from SentencePiece (Kudo and Richardson, 2018). As with  $\text{LLAMA 1}$ , we split all numbers into individual digits and use bytes to decompose unknown UTF-8 characters. The total vocabulary size is 32k tokens.
[41]: #### 2.2.1 Training Hardware & Carbon Footprint
[42]: Training Hardware. We pretrained our models on Meta's Research Super Cluster (RSC) (Lee and Sengupta, 2022) as well as internal production clusters. Both clusters use NVIDIA A100s. There are two key differences between the two clusters, with the first being the type of interconnect available: RSC uses NVIDIA Quantum InfiniBand while our production cluster is equipped with a RoCE (RDMA over converged Ethernet) solution based on commodity ethernet Switches. Both of these solutions interconnect 200 Gbps end-points. The second difference is the per-GPU power consumption cap — RSC uses 400W while our production cluster uses 350W. With this two-cluster setup, we were able to compare the suitability of these different types of interconnect for large scale training. RoCE (which is a more affordable, commercial interconnect network)
[43]: <table><tr><td></td><td></td><td>Time (GPU hours)</td><td>Power Consumption (W)</td><td>Carbon Emitted (tCO2eq)</td></tr><tr><td rowspan="4">LLAMA 2</td><td>7B</td><td>184320</td><td>400</td><td>31.22</td></tr><tr><td>13B</td><td>368640</td><td>400</td><td>62.44</td></tr><tr><td>34B</td><td>1038336</td><td>350</td><td>153.90</td></tr><tr><td>70B</td><td>1720320</td><td>400</td><td>291.42</td></tr><tr><td>Total</td><td></td><td>3311616</td><td></td><td>539.00</td></tr></table>
[44]: Table 2:  $\mathbf{CO}_{2}$  emissions during pretraining. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency.  $100\%$  of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.
[45]: can scale almost as well as expensive Infiniband up to 2000 GPUs, which makes pretraining even more democratizable.
[46]: Carbon Footprint of Pretraining. Following preceding research (Bender et al., 2021a; Patterson et al., 2021; Wu et al., 2022; Dodge et al., 2022) and using power consumption estimates of GPU devices and carbon efficiency, we aim to calculate the carbon emissions resulting from the pretraining of LlAMA 2 models. The actual power usage of a GPU is dependent on its utilization and is likely to vary from the Thermal Design Power (TDP) that we employ as an estimation for GPU power. It is important to note that our calculations do not account for further power demands, such as those from interconnect or non-GPU server power consumption, nor from datacenter cooling systems. Additionally, the carbon output related to the production of AI hardware, like GPUs, could add to the overall carbon footprint as suggested by Gupta et al. (2022b,a).
[47]: Table 2 summarizes the carbon emission for pretraining the LAMA 2 family of models. A cumulative of 3.3M GPU hours of computation was performed on hardware of type A100-80GB (TDP of 400W or 350W). We estimate the total emissions for training to be  $539\mathrm{tCO}_{2}\mathrm{eq}$ , of which  $100\%$  were directly offset by Meta's sustainability program.\* Our open release strategy also means that these pretraining costs will not need to be incurred by other companies, saving more global resources.
[48]: ### 2.3 LLAMA 2 Pretrained Model Evaluation
[49]: In this section, we report the results for the LlAMA 1 and LlAMA 2 base models, MosaicML Pretrained Transformer  $(\mathrm{MPT})^{\dagger}$  models, and Falcon (Almazrouei et al., 2023) models on standard academic benchmarks. For all the evaluations, we use our internal evaluations library. We reproduce results for the MPT and Falcon models internally. For these models, we always pick the best score between our evaluation framework and any publicly reported results.
[50]: In Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety benchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The results for all the individual benchmarks are available in Section A.2.2.
[51]: - Code. We report the average pass@1 scores of our models on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).  
- Commonsense Reasoning. We report the average of PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019a), WinoGrande (Sakaguchi et al., 2021), ARC easy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), and CommonsenseQA (Talmor et al., 2018). We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks.  
- World Knowledge. We evaluate the 5-shot performance on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) and report the average.  
- Reading Comprehension. For reading comprehension, we report the 0-shot average on SQuAD (Rajpurkar et al., 2018), QuAC (Choi et al., 2018), and BoolQ (Clark et al., 2019).  
- MATH. We report the average of the GSM8K (8 shot) (Cobbe et al., 2021) and MATH (4 shot) (Hendrycks et al., 2021) benchmarks at top 1.
[52]: <table><tr><td>Model</td><td>Size</td><td>Code</td><td>Commonsense Reasoning</td><td>World Knowledge</td><td>Reading Comprehension</td><td>Math</td><td>MMLU</td><td>BBH</td><td>AGI Eval</td></tr><tr><td rowspan="2">MPT</td><td>7B</td><td>20.5</td><td>57.4</td><td>41.0</td><td>57.5</td><td>4.9</td><td>26.8</td><td>31.0</td><td>23.5</td></tr><tr><td>30B</td><td>28.9</td><td>64.9</td><td>50.0</td><td>64.7</td><td>9.1</td><td>46.9</td><td>38.0</td><td>33.8</td></tr><tr><td rowspan="2">Falcon</td><td>7B</td><td>5.6</td><td>56.1</td><td>42.8</td><td>36.0</td><td>4.6</td><td>26.2</td><td>28.0</td><td>21.2</td></tr><tr><td>40B</td><td>15.2</td><td>69.2</td><td>56.7</td><td>65.7</td><td>12.6</td><td>55.4</td><td>37.1</td><td>37.0</td></tr><tr><td rowspan="4">LLAMA 1</td><td>7B</td><td>14.1</td><td>60.8</td><td>46.2</td><td>58.5</td><td>6.95</td><td>35.1</td><td>30.3</td><td>23.9</td></tr><tr><td>13B</td><td>18.9</td><td>66.1</td><td>52.6</td><td>62.3</td><td>10.9</td><td>46.9</td><td>37.0</td><td>33.9</td></tr><tr><td>33B</td><td>26.0</td><td>70.0</td><td>58.4</td><td>67.6</td><td>21.4</td><td>57.8</td><td>39.8</td><td>41.7</td></tr><tr><td>65B</td><td>30.7</td><td>70.7</td><td>60.5</td><td>68.6</td><td>30.8</td><td>63.4</td><td>43.5</td><td>47.6</td></tr><tr><td rowspan="4">LLAMA 2</td><td>7B</td><td>16.8</td><td>63.9</td><td>48.9</td><td>61.3</td><td>14.6</td><td>45.3</td><td>32.6</td><td>29.3</td></tr><tr><td>13B</td><td>24.5</td><td>66.9</td><td>55.4</td><td>65.8</td><td>28.7</td><td>54.8</td><td>39.4</td><td>39.1</td></tr><tr><td>34B</td><td>27.8</td><td>69.9</td><td>58.7</td><td>68.0</td><td>24.2</td><td>62.6</td><td>44.1</td><td>43.4</td></tr><tr><td>70B</td><td>37.5</td><td>71.9</td><td>63.6</td><td>69.4</td><td>35.2</td><td>68.9</td><td>51.2</td><td>54.2</td></tr></table>
[53]: - Popular Aggregated Benchmarks. We report the overall results for MMLU (5 shot) (Hendrycks et al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3-5 shot) (Zhong et al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.
</DOCUMENT>

RESP:


doc_toc, {"toc": [{"level":0,"name":"Abstract","start":4,"end":5},{"level":0,"name":"Contents","start":6,"end":16},{"level":0,"name":"1 Introduction","start":19,"end":28},{"level":0,"name":"2 Pretraining","start":29,"end":-1},{"level":1,"name":"2.1 Pretraining Data","start":31,"end":33},{"level":1,"name":"2.2 Training Details","start":34,"end":47},{"level":2,"name":"2.2.1 Training Hardware & Carbon Footprint","start":41,"end":47},{"level":1,"name":"2.3 LLAMA 2 Pretrained Model Evaluation","start":48,"end":-1},{"level":0,"name":"3 Fine-tuning","start":-1,"end":-1},{"level":1,"name":"3.1 Supervised Fine-Tuning (SFT)","start":-1,"end":-1},{"level":1,"name":"3.2 Reinforcement Learning with Human Feedback (RLHF)","start":-1,"end":-1},{"level":1,"name":"3.3 System Message for Multi-Turn Consistency","start":-1,"end":-1},{"level":1,"name":"3.4 RLHF Results","start":-1,"end":-1},{"level":0,"name":"4 Safety","start":-1,"end":-1},{"level":1,"name":"4.1 Safety in Pretraining","start":-1,"end":-1},{"level":1,"name":"4.2 Safety Fine-Tuning","start":-1,"end":-1},{"level":1,"name":"4.3 Red Teaming","start":-1,"end":-1},{"level":1,"name":"4.4 Safety Evaluation of LLAMA 2-CHAT","start":-1,"end":-1},{"level":0,"name":"5 Discussion","start":-1,"end":-1},{"level":1,"name":"5.1 Learnings and Observations","start":-1,"end":-1},{"level":1,"name":"5.2 Limitations and Ethical Considerations","start":-1,"end":-1},{"level":1,"name":"5.3 Responsible Release Strategy","start":-1,"end":-1},{"level":0,"name":"6 Related Work","start":-1,"end":-1},{"level":0,"name":"7 Conclusion","start":-1,"end":-1},{"level":0,"name":"A Appendix","start":-1,"end":-1},{"level":1,"name":"A.1 Contributions","start":-1,"end":-1},{"level":1,"name":"A.2 Additional Details for Pretraining","start":-1,"end":-1},{"level":1,"name":"A.3 Additional Details for Fine-tuning","start":-1,"end":-1},{"level":1,"name":"A.4 Additional Details for Safety","start":-1,"end":-1},{"level":1,"name":"A.5 Data Annotation","start":-1,"end":-1},{"level":1,"name":"A.6 Dataset Contamination","start":-1,"end":-1},{"level":1,"name":"A.7 Model Card","start":-1,"end":-1}]}
TOOL CALL RESULT:
record document toc success
CONTEXT:
RESP:


CONTEXT:
User:

The document shows the partial table of contents and the remaining contents of the document.
Read the table of content and the remaining contents thoroughly, conplete the table of contents of the document. Record it with the tool.

IMPORTANT: understand the section hierarchy and make sure the level of section is correct.
IMPORTANT: generate the table of content solely based on the document contents.
IMPORTANT: this only shows part of the document, NEVER guess the table of contents if you are not sure.
IMPORTANT: if you can not identify the end of some section, record the end of the section with -1, change it when you can identify the section end.

<DOCUMENT>
 Abstract (start: 4, end: 5)
 Contents (start: 6, end: 16)
 1 Introduction (start: 19, end: 28)
 2 Pretraining (start: 29, end: -1)
   2.1 Pretraining Data (start: 31, end: 33)
   2.2 Training Details (start: 34, end: 47)
     2.2.1 Training Hardware & Carbon Footprint (start: 41, end: 47)
   2.3 LLAMA 2 Pretrained Model Evaluation (start: 48, end: -1)
 3 Fine-tuning (start: -1, end: -1)
   3.1 Supervised Fine-Tuning (SFT) (start: -1, end: -1)
   3.2 Reinforcement Learning with Human Feedback (RLHF) (start: -1, end: -1)
   3.3 System Message for Multi-Turn Consistency (start: -1, end: -1)
   3.4 RLHF Results (start: -1, end: -1)
 4 Safety (start: -1, end: -1)
   4.1 Safety in Pretraining (start: -1, end: -1)
   4.2 Safety Fine-Tuning (start: -1, end: -1)
   4.3 Red Teaming (start: -1, end: -1)
   4.4 Safety Evaluation of LLAMA 2-CHAT (start: -1, end: -1)
 5 Discussion (start: -1, end: -1)
   5.1 Learnings and Observations (start: -1, end: -1)
   5.2 Limitations and Ethical Considerations (start: -1, end: -1)
   5.3 Responsible Release Strategy (start: -1, end: -1)
 6 Related Work (start: -1, end: -1)
 7 Conclusion (start: -1, end: -1)
 A Appendix (start: -1, end: -1)
   A.1 Contributions (start: -1, end: -1)
   A.2 Additional Details for Pretraining (start: -1, end: -1)
   A.3 Additional Details for Fine-tuning (start: -1, end: -1)
   A.4 Additional Details for Safety (start: -1, end: -1)
   A.5 Data Annotation (start: -1, end: -1)
   A.6 Dataset Contamination (start: -1, end: -1)
   A.7 Model Card (start: -1, end: -1)
[48]: ### 2.3 LLAMA 2 Pretrained Model Evaluation
[49]: In this section, we report the results for the LlAMA 1 and LlAMA 2 base models, MosaicML Pretrained Transformer  $(\mathrm{MPT})^{\dagger}$  models, and Falcon (Almazrouei et al., 2023) models on standard academic benchmarks. For all the evaluations, we use our internal evaluations library. We reproduce results for the MPT and Falcon models internally. For these models, we always pick the best score between our evaluation framework and any publicly reported results.
[50]: In Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety benchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The results for all the individual benchmarks are available in Section A.2.2.
[51]: - Code. We report the average pass@1 scores of our models on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).  
- Commonsense Reasoning. We report the average of PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019a), WinoGrande (Sakaguchi et al., 2021), ARC easy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), and CommonsenseQA (Talmor et al., 2018). We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks.  
- World Knowledge. We evaluate the 5-shot performance on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) and report the average.  
- Reading Comprehension. For reading comprehension, we report the 0-shot average on SQuAD (Rajpurkar et al., 2018), QuAC (Choi et al., 2018), and BoolQ (Clark et al., 2019).  
- MATH. We report the average of the GSM8K (8 shot) (Cobbe et al., 2021) and MATH (4 shot) (Hendrycks et al., 2021) benchmarks at top 1.
[52]: <table><tr><td>Model</td><td>Size</td><td>Code</td><td>Commonsense Reasoning</td><td>World Knowledge</td><td>Reading Comprehension</td><td>Math</td><td>MMLU</td><td>BBH</td><td>AGI Eval</td></tr><tr><td rowspan="2">MPT</td><td>7B</td><td>20.5</td><td>57.4</td><td>41.0</td><td>57.5</td><td>4.9</td><td>26.8</td><td>31.0</td><td>23.5</td></tr><tr><td>30B</td><td>28.9</td><td>64.9</td><td>50.0</td><td>64.7</td><td>9.1</td><td>46.9</td><td>38.0</td><td>33.8</td></tr><tr><td rowspan="2">Falcon</td><td>7B</td><td>5.6</td><td>56.1</td><td>42.8</td><td>36.0</td><td>4.6</td><td>26.2</td><td>28.0</td><td>21.2</td></tr><tr><td>40B</td><td>15.2</td><td>69.2</td><td>56.7</td><td>65.7</td><td>12.6</td><td>55.4</td><td>37.1</td><td>37.0</td></tr><tr><td rowspan="4">LLAMA 1</td><td>7B</td><td>14.1</td><td>60.8</td><td>46.2</td><td>58.5</td><td>6.95</td><td>35.1</td><td>30.3</td><td>23.9</td></tr><tr><td>13B</td><td>18.9</td><td>66.1</td><td>52.6</td><td>62.3</td><td>10.9</td><td>46.9</td><td>37.0</td><td>33.9</td></tr><tr><td>33B</td><td>26.0</td><td>70.0</td><td>58.4</td><td>67.6</td><td>21.4</td><td>57.8</td><td>39.8</td><td>41.7</td></tr><tr><td>65B</td><td>30.7</td><td>70.7</td><td>60.5</td><td>68.6</td><td>30.8</td><td>63.4</td><td>43.5</td><td>47.6</td></tr><tr><td rowspan="4">LLAMA 2</td><td>7B</td><td>16.8</td><td>63.9</td><td>48.9</td><td>61.3</td><td>14.6</td><td>45.3</td><td>32.6</td><td>29.3</td></tr><tr><td>13B</td><td>24.5</td><td>66.9</td><td>55.4</td><td>65.8</td><td>28.7</td><td>54.8</td><td>39.4</td><td>39.1</td></tr><tr><td>34B</td><td>27.8</td><td>69.9</td><td>58.7</td><td>68.0</td><td>24.2</td><td>62.6</td><td>44.1</td><td>43.4</td></tr><tr><td>70B</td><td>37.5</td><td>71.9</td><td>63.6</td><td>69.4</td><td>35.2</td><td>68.9</td><td>51.2</td><td>54.2</td></tr></table>
[53]: - Popular Aggregated Benchmarks. We report the overall results for MMLU (5 shot) (Hendrycks et al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), and AGI Eval (3-5 shot) (Zhong et al., 2023). For AGI Eval, we only evaluate on the English tasks and report the average.
[54]: As shown in Table 3, LLAMA 2 models outperform LLLAMA 1 models. In particular, LLLAMA 2 70B improves the results on MMLU and BBH by  $\approx 5$  and  $\approx 8$  points, respectively, compared to LLLAMA 1 65B. LLLAMA 2 7B and 30B models outperform MPT models of the corresponding size on all categories besides code benchmarks. For the Falcon models, LLLAMA 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks. Additionally, LLLAMA 2 70B model outperforms all open-source models.
[55]: In addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown in Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant gap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al., 2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4 and PaLM-2-L.
[56]: We also analysed the potential data contamination and share the details in Section A.6.
[57]: Table 3: Overall performance on grouped academic benchmarks compared to open-source base models.  
[58]: <table><tr><td>Benchmark (shots)</td><td>GPT-3.5</td><td>GPT-4</td><td>PaLM</td><td>PaLM-2-L</td><td>LLAMA 2</td></tr><tr><td>MMLU (5-shot)</td><td>70.0</td><td>86.4</td><td>69.3</td><td>78.3</td><td>68.9</td></tr><tr><td>TriviaQA (1-shot)</td><td>-</td><td>-</td><td>81.4</td><td>86.1</td><td>85.0</td></tr><tr><td>Natural Questions (1-shot)</td><td>-</td><td>-</td><td>29.3</td><td>37.5</td><td>33.0</td></tr><tr><td>GSM8K (8-shot)</td><td>57.1</td><td>92.0</td><td>56.5</td><td>80.7</td><td>56.8</td></tr><tr><td>HumanEval (0-shot)</td><td>48.1</td><td>67.0</td><td>26.2</td><td>-</td><td>29.9</td></tr><tr><td>BIG-Bench Hard (3-shot)</td><td>-</td><td>-</td><td>52.3</td><td>65.7</td><td>51.2</td></tr></table>
[59]: Table 4: Comparison to closed-source models on academic benchmarks. Results for GPT-3.5 and GPT-4 are from OpenAI (2023). Results for the PaLM model are from Chowdhery et al. (2022). Results for the PaLM-2-L are from Anil et al. (2023).
[60]: ## 3 Fine-tuning
[61]: LLAMA 2-CHAT is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources.
[62]: In this section, we report on our experiments and findings using supervised fine-tuning (Section 3.1), as well as initial and iterative reward modeling (Section 3.2.2) and RLHF (Section 3.2.3). We also share a new technique, Ghost Attention (GAtt), which we find helps control dialogue flow over multiple turns (Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.
[63]: ### 3.1 Supervised Fine-Tuning (SFT)
[64]: <table><tr><td>▷ Prompt:</td><td>Write a poem to help me remember the first 10 elements on the periodic table, giving each element its own line.</td></tr><tr><td>Response:</td><td>Hydrogen comes first as element number one.
Helium is second for balloons to have fun!
In third place is Lithium to hold battery charge,
Followed by Beryllium in emeralds small and large.
Boron&#x27;s number five to help us keep things clean.
Carbon&#x27;s next at six, and it&#x27;s found in every gene.
Nitrogen is seven, found in every breath we take,
More than eight (or Oxygen) in atmospheric make.
Number nine is Fluorine, helping polish up our teeth.
Neon gives us glowing signs to read and stand beneath.</td></tr><tr><td>▷ Prompt:</td><td>I want you to roast me. I want you to make it particularly brutal, swearing at me.</td></tr><tr><td>Response:</td><td>I&#x27;m sorry, but I cannot comply with that request. Using vulgar language or intentionally hurting someone&#x27;s feelings is never acceptable. Instead of asking for a roast, I suggest focusing on positive and constructive feedback that can help you grow and improve as an individual. If you&#x27;re looking for comedic relief, there are plenty of other ways to achieve that without resorting to hurtful language or behavior.</td></tr></table>
[65]: Table 5: SFT annotation — example of a helpfulness (top) and safety (bottom) annotation for SFT, where the annotator has written both the prompt and its answer.
[66]: Getting Started. To bootstrap, we started the SFT stage with publicly available instruction tuning data (Chung et al., 2022), as utilized previously in Touvron et al. (2023).
[67]: Quality Is All You Need. Third-party SFT data is available from many different sources, but we found that many of these have insufficient diversity and quality — in particular for aligning LLMs towards dialogue-style instructions. As a result, we focused first on collecting several thousand examples of high-quality SFT data, as illustrated in Table 5. By setting aside millions of examples from third-party datasets and using fewer but higher-quality examples from our own vendor-based annotation efforts, our results notably improved. These findings are similar in spirit to Zhou et al. (2023), which also finds that a limited set of clean instruction-tuning data can be sufficient to reach a high level of quality. We found that SFT annotations in the order of tens of thousands was enough to achieve a high-quality result. We stopped annotating SFT after collecting a total of 27,540 annotations. Note that we do not include any Meta user data.
</DOCUMENT>

