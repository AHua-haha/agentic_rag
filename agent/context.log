User:


Instructions:
1. You will be given a list of continuous text chunks.
2. Analyze the content, meaning, and themes of each chunk.
3. Identify which chunks relate to each other and form coherent meaning. and group these chunks.
4. Output each group and the chunk id in the group.

Additional rules:
- Do NOT merge chunks unless they clearly share meaning, topic, or purpose.
- Every chunk must appear in exactly one group.
- If a chunk does not fit any group, create a new one for it.
- Maintain original chunk numbering.
- DO NOT group the headings.

Now process the following chunks:
<CHUNKS>
[0]: # LLAMA 2: Open Foundation and Fine-Tuned Chat Models
[1]: Hugo Touvron* Louis Martin† Kevin Stone†
[2]: Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra  
Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen  
Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller  
Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou  
Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev  
Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich  
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra  
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi  
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang  
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang  
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic  
Sergey Edunov Thomas Scialom*
[3]: ## GenAI,Meta
[4]: ## Abstract
[5]: In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called  $\text{LLAMA} \, 2-\text{CHAT}$ , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-CHAT in order to enable the community to build on our work and contribute to the responsible development of LLMs.
[6]: ## Contents
[7]: 1 Introduction 3  
2 Pretraining 5
[8]: 2.1 Pretraining Data 5  
2.2 Training Details 5  
2.3 LLAMA 2 Pretrained Model Evaluation 7
[9]: 3 Fine-tuning 8
[10]: 3.1 Supervised Fine-Tuning (SFT) 9  
3.2 Reinforcement Learning with Human Feedback (RLHF) 9  
3.3 System Message for Multi-Turn Consistency 16  
3.4 RLHF Results 17
[11]: 4 Safety 20
[12]: 4.1 Safety in Pretraining 20  
4.2 Safety Fine-Tuning 23  
4.3 Red Teaming 28  
4.4 Safety Evaluation of LLAMA 2-CHAT 29
[13]: 5 Discussion 32
[14]: 5.1 Learnings and Observations 32  
5.2 Limitations and Ethical Considerations 34  
5.3 Responsible Release Strategy 35
[15]: 6 Related Work 35  
7 Conclusion 36  
A Appendix 46
[16]: A.1 Contributions 46  
A.2 Additional Details for Pretraining 47  
A.3 Additional Details for Fine-tuning 51  
A.4 Additional Details for Safety 58  
A.5 Data Annotation 72  
A.6 Dataset Contamination 75  
A.7 Model Card 77
[17]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/efb40aa2874ba2af90fd13e297e03de151aa27d82aa3a701dc9cb341dc7f1f3b.jpg)  
Figure 1: Helpfulness human evaluation results for LLAMA 2-CHAT compared to other open-source and closed-source models. Human raters compared model generations on  $\sim 4k$  prompts consisting of both single and multi-turn prompts. The  $95\%$  confidence intervals for this evaluation are between  $1\%$  and  $2\%$ . More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent difficulty of comparing generations.
[18]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/08f6863778b4206402cec247ce6410b9fcb0e70bd55573ad296b7ec92db20a46.jpg)  
Figure 2: Win-rate % for helpfulness and safety between commercial-licensed baselines and LAMA 2-CHAT, according to GPT-4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.
[19]: ## 1 Introduction
[20]: Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.
[21]: The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022), but none of these models are suitable substitutes for closed "product" LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.
[22]: In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs,  $LLAMA$  2 and  $LLAMA$  2-CHAT, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,  $LLAMA$  2-CHAT models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of  $LLAMA$  2 and  $LLAMA$  2-CHAT, such as the emergence of tool usage and temporal organization of knowledge.
[23]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/c8fd7ef2dcd67577f41264332a402eb3c7eb9ea4614d0af1a0144a2bc2c511b6.jpg)  
Figure 3: Safety human evaluation results for LLM2-CHAT compared to other open-source and closed-source models. Human raters judged model generations for safety violations across  $\sim 2,000$  adversarial prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the LLM2-CHAT models.
[24]: We are releasing the following models to the general public for research and commercial use:

** 558 paragraphs remaining **
</CHUNKS>

User:


Instructions:
1. You will be given a list of continuous text chunks.
2. Analyze the content, meaning, and themes of each chunk.
3. Identify which chunks relate to each other and form coherent meaning. and group these chunks.
4. Output each group and the chunk id in the group.

Additional rules:
- Do NOT merge chunks unless they clearly share meaning, topic, or purpose.
- Every chunk must appear in exactly one group.
- If a chunk does not fit any group, create a new one for it.
- Maintain original chunk numbering.
- DO NOT group the headings.

Now process the following chunks:
<CHUNKS>
[0]: # LLAMA 2: Open Foundation and Fine-Tuned Chat Models
[1]: Hugo Touvron* Louis Martin† Kevin Stone†
[2]: Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra  
Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen  
Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller  
Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou  
Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev  
Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich  
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra  
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi  
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang  
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang  
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic  
Sergey Edunov Thomas Scialom*
[3]: ## GenAI,Meta
[4]: ## Abstract
[5]: In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called  $\text{LLAMA} \, 2-\text{CHAT}$ , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-CHAT in order to enable the community to build on our work and contribute to the responsible development of LLMs.
[6]: ## Contents
[7]: 1 Introduction 3  
2 Pretraining 5
[8]: 2.1 Pretraining Data 5  
2.2 Training Details 5  
2.3 LLAMA 2 Pretrained Model Evaluation 7
[9]: 3 Fine-tuning 8
[10]: 3.1 Supervised Fine-Tuning (SFT) 9  
3.2 Reinforcement Learning with Human Feedback (RLHF) 9  
3.3 System Message for Multi-Turn Consistency 16  
3.4 RLHF Results 17
[11]: 4 Safety 20
[12]: 4.1 Safety in Pretraining 20  
4.2 Safety Fine-Tuning 23  
4.3 Red Teaming 28  
4.4 Safety Evaluation of LLAMA 2-CHAT 29
[13]: 5 Discussion 32
[14]: 5.1 Learnings and Observations 32  
5.2 Limitations and Ethical Considerations 34  
5.3 Responsible Release Strategy 35
[15]: 6 Related Work 35  
7 Conclusion 36  
A Appendix 46
[16]: A.1 Contributions 46  
A.2 Additional Details for Pretraining 47  
A.3 Additional Details for Fine-tuning 51  
A.4 Additional Details for Safety 58  
A.5 Data Annotation 72  
A.6 Dataset Contamination 75  
A.7 Model Card 77
[17]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/efb40aa2874ba2af90fd13e297e03de151aa27d82aa3a701dc9cb341dc7f1f3b.jpg)  
Figure 1: Helpfulness human evaluation results for LLAMA 2-CHAT compared to other open-source and closed-source models. Human raters compared model generations on  $\sim 4k$  prompts consisting of both single and multi-turn prompts. The  $95\%$  confidence intervals for this evaluation are between  $1\%$  and  $2\%$ . More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent difficulty of comparing generations.
[18]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/08f6863778b4206402cec247ce6410b9fcb0e70bd55573ad296b7ec92db20a46.jpg)  
Figure 2: Win-rate % for helpfulness and safety between commercial-licensed baselines and LAMA 2-CHAT, according to GPT-4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.
[19]: ## 1 Introduction
[20]: Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.
[21]: The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022), but none of these models are suitable substitutes for closed "product" LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.
[22]: In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs,  $LLAMA$  2 and  $LLAMA$  2-CHAT, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,  $LLAMA$  2-CHAT models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of  $LLAMA$  2 and  $LLAMA$  2-CHAT, such as the emergence of tool usage and temporal organization of knowledge.
[23]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/c8fd7ef2dcd67577f41264332a402eb3c7eb9ea4614d0af1a0144a2bc2c511b6.jpg)  
Figure 3: Safety human evaluation results for LLM2-CHAT compared to other open-source and closed-source models. Human raters judged model generations for safety violations across  $\sim 2,000$  adversarial prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the LLM2-CHAT models.
[24]: We are releasing the following models to the general public for research and commercial use:

** 558 paragraphs remaining **
</CHUNKS>

RESP:
# Document Structure Analysis

Based on my analysis of the chunks, here are the semantically coherent groups:

## Group 1: Title and Authors
**Chunks:** [0], [1], [2], [3]
- Main title of the paper
- Author names (primary and full author list)
- Institutional affiliation

## Group 2: Abstract
**Chunks:** [4], [5]
- Abstract heading and content describing the work, models, and key contributions

## Group 3: Table of Contents
**Chunks:** [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16]
- Complete table of contents with section numbers and page references
- Includes main sections (Introduction through Conclusion) and appendix

## Group 4: Introduction Section
**Chunks:** [19], [20], [21], [22]
- Introduction heading and full introduction content
- Discussion of LLM capabilities, existing models, and motivation for Llama 2

## Group 5: Evaluation Results and Figures
**Chunks:** [17], [18], [23]
- Figures 1, 2, and 3 showing human evaluation results
- Helpfulness and safety comparison benchmarks with confidence intervals and methodology notes

## Group 6: Model Release Statement
**Chunks:** [24]
- Opening statement about releasing models to the public

CONTEXT:
RESP:


CONTEXT:
RESP:


CONTEXT:
User:


Instructions:
1. You will be given a list of continuous text chunks.
2. Analyze the content, meaning, and themes of each chunk.
3. Identify which chunks relate to each other and form coherent meaning. and group these chunks.
4. Output each group and the chunk id in the group.

Additional rules:
- Do NOT merge chunks unless they clearly share meaning, topic, or purpose.
- Every chunk must appear in exactly one group.
- If a chunk does not fit any group, create a new one for it.
- Maintain original chunk numbering.
- DO NOT group the headings.

Now process the following chunks:
<CHUNKS>
[0]: # LLAMA 2: Open Foundation and Fine-Tuned Chat Models
[1]: Hugo Touvron* Louis Martin† Kevin Stone†
[2]: Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra  
Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen  
Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller  
Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou  
Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev  
Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich  
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra  
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi  
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang  
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang  
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic  
Sergey Edunov Thomas Scialom*
[3]: ## GenAI,Meta
[4]: ## Abstract
[5]: In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called  $\text{LLAMA} \, 2-\text{CHAT}$ , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-CHAT in order to enable the community to build on our work and contribute to the responsible development of LLMs.
[6]: ## Contents
[7]: 1 Introduction 3  
2 Pretraining 5
[8]: 2.1 Pretraining Data 5  
2.2 Training Details 5  
2.3 LLAMA 2 Pretrained Model Evaluation 7
[9]: 3 Fine-tuning 8
[10]: 3.1 Supervised Fine-Tuning (SFT) 9  
3.2 Reinforcement Learning with Human Feedback (RLHF) 9  
3.3 System Message for Multi-Turn Consistency 16  
3.4 RLHF Results 17
[11]: 4 Safety 20
[12]: 4.1 Safety in Pretraining 20  
4.2 Safety Fine-Tuning 23  
4.3 Red Teaming 28  
4.4 Safety Evaluation of LLAMA 2-CHAT 29
[13]: 5 Discussion 32
[14]: 5.1 Learnings and Observations 32  
5.2 Limitations and Ethical Considerations 34  
5.3 Responsible Release Strategy 35
[15]: 6 Related Work 35  
7 Conclusion 36  
A Appendix 46
[16]: A.1 Contributions 46  
A.2 Additional Details for Pretraining 47  
A.3 Additional Details for Fine-tuning 51  
A.4 Additional Details for Safety 58  
A.5 Data Annotation 72  
A.6 Dataset Contamination 75  
A.7 Model Card 77
[17]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/efb40aa2874ba2af90fd13e297e03de151aa27d82aa3a701dc9cb341dc7f1f3b.jpg)  
Figure 1: Helpfulness human evaluation results for LLAMA 2-CHAT compared to other open-source and closed-source models. Human raters compared model generations on  $\sim 4k$  prompts consisting of both single and multi-turn prompts. The  $95\%$  confidence intervals for this evaluation are between  $1\%$  and  $2\%$ . More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent difficulty of comparing generations.
[18]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/08f6863778b4206402cec247ce6410b9fcb0e70bd55573ad296b7ec92db20a46.jpg)  
Figure 2: Win-rate % for helpfulness and safety between commercial-licensed baselines and LAMA 2-CHAT, according to GPT-4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.
[19]: ## 1 Introduction
[20]: Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.
[21]: The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022), but none of these models are suitable substitutes for closed "product" LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.
[22]: In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs,  $LLAMA$  2 and  $LLAMA$  2-CHAT, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,  $LLAMA$  2-CHAT models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of  $LLAMA$  2 and  $LLAMA$  2-CHAT, such as the emergence of tool usage and temporal organization of knowledge.
[23]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/c8fd7ef2dcd67577f41264332a402eb3c7eb9ea4614d0af1a0144a2bc2c511b6.jpg)  
Figure 3: Safety human evaluation results for LLM2-CHAT compared to other open-source and closed-source models. Human raters judged model generations for safety violations across  $\sim 2,000$  adversarial prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the LLM2-CHAT models.
[24]: We are releasing the following models to the general public for research and commercial use:
[25]: 1. LLAMA 2, an updated version of LlAMA 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by  $40\%$ , doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of LlAMA 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing. $\S$  
2. LLMA 2-CHAT, a fine-tuned version of LLMA 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well.
[26]: We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, LAMA 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover all scenarios. Therefore, before deploying any applications of LAMA 2-CHAT, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guide<sup>®</sup> and code examples<sup>®</sup> to facilitate the safe deployment of LAMA 2 and LAMA 2-CHAT. More details of our responsible release strategy can be found in Section 5.3.
[27]: The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology (Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related work (Section 6), and conclusions (Section 7).
[28]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/3e2b7379f53cb7a2d0215aa5335b7b488fcc8b1bb979e5997227df401065e2a0.jpg)  
Figure 4: Training of LlAMA 2-CHAT: This process begins with the pretraining of LlAMA 2 using publicly available online sources. Following this, we create an initial version of LlAMA 2-CHAT through the application of supervised fine-tuning. Subsequently, the model is iteratively refined using Reinforcement Learning with Human Feedback (RLHF) methodologies, specifically through rejection sampling and Proximal Policy Optimization (PPO). Throughout the RLHF stage, the accumulation of iterative reward modeling data in parallel with model enhancements is crucial to ensure the reward models remain within distribution.
[29]: ## 2 Pretraining

** 553 paragraphs remaining **
</CHUNKS>

RESP:
# Document Structure Analysis

Based on my analysis of the chunks, here are the coherent groups:

## Group 1: Title and Publication Info
**Chunks:** [0], [1], [2], [3]
*Rationale: Document title, author names, and affiliation*

## Group 2: Abstract
**Chunks:** [4], [5]
*Rationale: Abstract section describing the work overview*

## Group 3: Table of Contents
**Chunks:** [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16]
*Rationale: Document structure and section listings*

## Group 4: Introduction Section
**Chunks:** [19], [20], [21], [22], [23], [24], [25], [26], [27]
*Rationale: Introduction content discussing LLMs, Llama 2 development, model release, and safety considerations*

## Group 5: Figures and Results
**Chunks:** [17], [18], [28]
*Rationale: Evaluation figures and training pipeline diagrams with captions*

## Group 6: Pretraining Section Header
**Chunks:** [29]
*Rationale: Section 2 heading for Pretraining*

CONTEXT:
RESP:


CONTEXT:
User:


Instructions:
1. You will be given a list of continuous text chunks.
2. Analyze the content, meaning, and themes of each chunk.
3. Identify which chunks relate to each other and form coherent meaning. and group these chunks.
4. Output each group and the chunk id in the group.

Additional rules:
- Do NOT merge chunks unless they clearly share meaning, topic, or purpose.
- Every chunk must appear in exactly one group.
- If a chunk does not fit any group, create a new one for it.
- Maintain original chunk numbering.
- DO NOT group the headings.

Now process the following chunks:
<CHUNKS>
[0]: # LLAMA 2: Open Foundation and Fine-Tuned Chat Models
[1]: Hugo Touvron* Louis Martin† Kevin Stone†
[2]: Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra  
Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen  
Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller  
Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou  
Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev  
Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich  
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra  
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi  
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang  
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang  
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic  
Sergey Edunov Thomas Scialom*
[3]: ## GenAI,Meta
[4]: ## Abstract
[5]: In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called  $\text{LLAMA} \, 2-\text{CHAT}$ , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-CHAT in order to enable the community to build on our work and contribute to the responsible development of LLMs.
[6]: ## Contents
[7]: 1 Introduction 3  
2 Pretraining 5
[8]: 2.1 Pretraining Data 5  
2.2 Training Details 5  
2.3 LLAMA 2 Pretrained Model Evaluation 7
[9]: 3 Fine-tuning 8
[10]: 3.1 Supervised Fine-Tuning (SFT) 9  
3.2 Reinforcement Learning with Human Feedback (RLHF) 9  
3.3 System Message for Multi-Turn Consistency 16  
3.4 RLHF Results 17
[11]: 4 Safety 20
[12]: 4.1 Safety in Pretraining 20  
4.2 Safety Fine-Tuning 23  
4.3 Red Teaming 28  
4.4 Safety Evaluation of LLAMA 2-CHAT 29
[13]: 5 Discussion 32
[14]: 5.1 Learnings and Observations 32  
5.2 Limitations and Ethical Considerations 34  
5.3 Responsible Release Strategy 35
[15]: 6 Related Work 35  
7 Conclusion 36  
A Appendix 46
[16]: A.1 Contributions 46  
A.2 Additional Details for Pretraining 47  
A.3 Additional Details for Fine-tuning 51  
A.4 Additional Details for Safety 58  
A.5 Data Annotation 72  
A.6 Dataset Contamination 75  
A.7 Model Card 77
[17]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/efb40aa2874ba2af90fd13e297e03de151aa27d82aa3a701dc9cb341dc7f1f3b.jpg)  
Figure 1: Helpfulness human evaluation results for LLAMA 2-CHAT compared to other open-source and closed-source models. Human raters compared model generations on  $\sim 4k$  prompts consisting of both single and multi-turn prompts. The  $95\%$  confidence intervals for this evaluation are between  $1\%$  and  $2\%$ . More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent difficulty of comparing generations.
[18]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/08f6863778b4206402cec247ce6410b9fcb0e70bd55573ad296b7ec92db20a46.jpg)  
Figure 2: Win-rate % for helpfulness and safety between commercial-licensed baselines and LAMA 2-CHAT, according to GPT-4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.
[19]: ## 1 Introduction
[20]: Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.
[21]: The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022), but none of these models are suitable substitutes for closed "product" LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.
[22]: In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs,  $LLAMA$  2 and  $LLAMA$  2-CHAT, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,  $LLAMA$  2-CHAT models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of  $LLAMA$  2 and  $LLAMA$  2-CHAT, such as the emergence of tool usage and temporal organization of knowledge.
[23]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/c8fd7ef2dcd67577f41264332a402eb3c7eb9ea4614d0af1a0144a2bc2c511b6.jpg)  
Figure 3: Safety human evaluation results for LLM2-CHAT compared to other open-source and closed-source models. Human raters judged model generations for safety violations across  $\sim 2,000$  adversarial prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the LLM2-CHAT models.
[24]: We are releasing the following models to the general public for research and commercial use:
[25]: 1. LLAMA 2, an updated version of LlAMA 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by  $40\%$ , doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of LlAMA 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing. $\S$  
2. LLMA 2-CHAT, a fine-tuned version of LLMA 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well.
[26]: We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, LAMA 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover all scenarios. Therefore, before deploying any applications of LAMA 2-CHAT, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guide<sup>®</sup> and code examples<sup>®</sup> to facilitate the safe deployment of LAMA 2 and LAMA 2-CHAT. More details of our responsible release strategy can be found in Section 5.3.
[27]: The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology (Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related work (Section 6), and conclusions (Section 7).
[28]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/3e2b7379f53cb7a2d0215aa5335b7b488fcc8b1bb979e5997227df401065e2a0.jpg)  
Figure 4: Training of LlAMA 2-CHAT: This process begins with the pretraining of LlAMA 2 using publicly available online sources. Following this, we create an initial version of LlAMA 2-CHAT through the application of supervised fine-tuning. Subsequently, the model is iteratively refined using Reinforcement Learning with Human Feedback (RLHF) methodologies, specifically through rejection sampling and Proximal Policy Optimization (PPO). Throughout the RLHF stage, the accumulation of iterative reward modeling data in parallel with model enhancements is crucial to ensure the reward models remain within distribution.
[29]: ## 2 Pretraining
[30]: To create the new family of LlAMA 2 models, we began with the pretraining approach described in Touvron et al. (2023), using an optimized auto-regressive transformer, but made several changes to improve performance. Specifically, we performed more robust data cleaning, updated our data mixes, trained on  $40\%$  more total tokens, doubled the context length, and used grouped-query attention (GQA) to improve inference scalability for our larger models. Table 1 compares the attributes of the new LlAMA 2 models with the LlAMA 1 models.
[31]: ### 2.1 Pretraining Data
[32]: Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta's products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance-cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.
[33]: We performed a variety of pretraining data investigations so that users can better understand the potential capabilities and limitations of our models; results can be found in Section 4.1.
[34]: ### 2.2 Training Details
[35]: We adopt most of the pretraining setting and model architecture from  $\text{LLAMA 1}$ . We use the standard transformer architecture (Vaswani et al., 2017), apply pre-normalization using RMSNorm (Zhang and Sennrich, 2019), use the SwiGLU activation function (Shazeer, 2020), and rotary positional embeddings (RoPE, Su et al. 2022). The primary architectural differences from  $\text{LLAMA 1}$  include increased context length and grouped-query attention (GQA). We detail in Appendix Section A.2.1 each of these differences with ablation experiments to demonstrate their importance.
[36]: Hyperparameters. We trained using the AdamW optimizer (Loshchilov and Hutter, 2017), with  $\beta_{1} = 0.9$ ,  $\beta_{2} = 0.95$ ,  $\mathrm{eps} = 10^{-5}$ . We use a cosine learning rate schedule, with warmup of 2000 steps, and decay final learning rate down to  $10\%$  of the peak learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. Figure 5 (a) shows the training loss for LAMA 2 with these hyperparameters.
[37]: <table><tr><td></td><td>Training Data</td><td>Params</td><td>Context Length</td><td>GQA</td><td>Tokens</td><td>LR</td></tr><tr><td rowspan="4">LLAMA 1</td><td rowspan="4">See Touvron et al. (2023)</td><td>7B</td><td>2k</td><td>×</td><td>1.0T</td><td>3.0 × 10-4</td></tr><tr><td>13B</td><td>2k</td><td>×</td><td>1.0T</td><td>3.0 × 10-4</td></tr><tr><td>33B</td><td>2k</td><td>×</td><td>1.4T</td><td>1.5 × 10-4</td></tr><tr><td>65B</td><td>2k</td><td>×</td><td>1.4T</td><td>1.5 × 10-4</td></tr><tr><td rowspan="4">LLAMA 2</td><td rowspan="4">A new mix of publicly available online data</td><td>7B</td><td>4k</td><td>×</td><td>2.0T</td><td>3.0 × 10-4</td></tr><tr><td>13B</td><td>4k</td><td>×</td><td>2.0T</td><td>3.0 × 10-4</td></tr><tr><td>34B</td><td>4k</td><td>✓</td><td>2.0T</td><td>1.5 × 10-4</td></tr><tr><td>70B</td><td>4k</td><td>✓</td><td>2.0T</td><td>1.5 × 10-4</td></tr></table>
[38]: Table 1: LlAMA 2 family of models. Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models — 34B and 70B — use Grouped-Query Attention (GQA) for improved inference scalability.
[39]: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/29a21b4a91d2682caa52a8c0bc4be1f403400f9c1ea590ae5bcd7f7894a6e0ff.jpg)  
Figure 5: Training Loss for LlAMA 2 models. We compare the training loss of the LlAMA 2 family of models. We observe that after pretraining on 2T Tokens, the models still did not show any sign of saturation.
[40]: Tokenizer. We use the same tokenizer as  $\text{LLAMA 1}$ ; it employs a bptein encoding (BPE) algorithm (Sennrich et al., 2016) using the implementation from SentencePiece (Kudo and Richardson, 2018). As with  $\text{LLAMA 1}$ , we split all numbers into individual digits and use bytes to decompose unknown UTF-8 characters. The total vocabulary size is 32k tokens.
[41]: #### 2.2.1 Training Hardware & Carbon Footprint
[42]: Training Hardware. We pretrained our models on Meta's Research Super Cluster (RSC) (Lee and Sengupta, 2022) as well as internal production clusters. Both clusters use NVIDIA A100s. There are two key differences between the two clusters, with the first being the type of interconnect available: RSC uses NVIDIA Quantum InfiniBand while our production cluster is equipped with a RoCE (RDMA over converged Ethernet) solution based on commodity ethernet Switches. Both of these solutions interconnect 200 Gbps end-points. The second difference is the per-GPU power consumption cap — RSC uses 400W while our production cluster uses 350W. With this two-cluster setup, we were able to compare the suitability of these different types of interconnect for large scale training. RoCE (which is a more affordable, commercial interconnect network)
[43]: <table><tr><td></td><td></td><td>Time (GPU hours)</td><td>Power Consumption (W)</td><td>Carbon Emitted (tCO2eq)</td></tr><tr><td rowspan="4">LLAMA 2</td><td>7B</td><td>184320</td><td>400</td><td>31.22</td></tr><tr><td>13B</td><td>368640</td><td>400</td><td>62.44</td></tr><tr><td>34B</td><td>1038336</td><td>350</td><td>153.90</td></tr><tr><td>70B</td><td>1720320</td><td>400</td><td>291.42</td></tr><tr><td>Total</td><td></td><td>3311616</td><td></td><td>539.00</td></tr></table>
[44]: Table 2:  $\mathbf{CO}_{2}$  emissions during pretraining. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency.  $100\%$  of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.
[45]: can scale almost as well as expensive Infiniband up to 2000 GPUs, which makes pretraining even more democratizable.
[46]: Carbon Footprint of Pretraining. Following preceding research (Bender et al., 2021a; Patterson et al., 2021; Wu et al., 2022; Dodge et al., 2022) and using power consumption estimates of GPU devices and carbon efficiency, we aim to calculate the carbon emissions resulting from the pretraining of LlAMA 2 models. The actual power usage of a GPU is dependent on its utilization and is likely to vary from the Thermal Design Power (TDP) that we employ as an estimation for GPU power. It is important to note that our calculations do not account for further power demands, such as those from interconnect or non-GPU server power consumption, nor from datacenter cooling systems. Additionally, the carbon output related to the production of AI hardware, like GPUs, could add to the overall carbon footprint as suggested by Gupta et al. (2022b,a).
[47]: Table 2 summarizes the carbon emission for pretraining the LAMA 2 family of models. A cumulative of 3.3M GPU hours of computation was performed on hardware of type A100-80GB (TDP of 400W or 350W). We estimate the total emissions for training to be  $539\mathrm{tCO}_{2}\mathrm{eq}$ , of which  $100\%$  were directly offset by Meta's sustainability program.\* Our open release strategy also means that these pretraining costs will not need to be incurred by other companies, saving more global resources.
[48]: ### 2.3 LLAMA 2 Pretrained Model Evaluation
[49]: In this section, we report the results for the LlAMA 1 and LlAMA 2 base models, MosaicML Pretrained Transformer  $(\mathrm{MPT})^{\dagger}$  models, and Falcon (Almazrouei et al., 2023) models on standard academic benchmarks. For all the evaluations, we use our internal evaluations library. We reproduce results for the MPT and Falcon models internally. For these models, we always pick the best score between our evaluation framework and any publicly reported results.

** 533 paragraphs remaining **
</CHUNKS>

RESP:
# Document Structure Analysis

Based on the provided chunks, here are the coherent semantic groups:

## Group 1: Title and Authorship
**Chunks: [0], [1], [2], [3]**
- Document title, author names, and affiliation information

## Group 2: Abstract and Overview
**Chunks: [4], [5]**
- Abstract summarizing the work on Llama 2 models and their capabilities

## Group 3: Table of Contents
**Chunks: [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16]**
- Complete table of contents with section numbers and page references

## Group 4: Introduction - Context and Motivation
**Chunks: [19], [20], [21]**
- Introduction section explaining the capabilities of LLMs and the motivation for Llama 2 development

## Group 5: Introduction - Model Release and Strategy
**Chunks: [22], [24], [25], [26]**
- Description of models being released and responsible release strategy

## Group 6: Introduction - Paper Overview
**Chunks: [27]**
- Paper structure and roadmap

## Group 7: Training Pipeline Overview
**Chunks: [28]**
- Visual representation and explanation of the training process

## Group 8: Pretraining - Data and Configuration
**Chunks: [29], [30], [31], [32], [33], [34]**
- Pretraining methodology, data sources, and approach description

## Group 9: Pretraining - Technical Details
**Chunks: [35], [36], [37], [38], [40]**
- Architecture specifications, hyperparameters, model configurations, and tokenizer information

## Group 10: Training Infrastructure and Environmental Impact
**Chunks: [41], [42], [43], [44], [45], [46], [47]**
- Hardware specifications and carbon footprint calculations

## Group 11: Model Evaluation Results
**Chunks: [17], [18], [23]**
- Human evaluation comparisons and performance figures

## Group 12: Evaluation Methodology
**Chunks: [33], [48], [49]**
- Information about evaluation approaches and benchmark testing

CONTEXT:
RESP:


CONTEXT:
RESP:


CONTEXT:
User:

Rules for chunking:
1. A chunk must represent a coherent idea, topic, or subtopic.
2. Do NOT split in the middle of a sentence or logical argument.
3. Prefer splitting at headings, subheadings, major paragraph breaks, or topic shifts.
4. Ideal chunk size: 300-600 words (or ~500-1200 tokens).
5. If a section is very long, split it into multiple coherent sub-chunks.
6. Preserve the original order of the text.

Output format:
id: 22
content: ...

id: 23
content: ...

Now chunk the following document:


<DOCUMENT>
# LLAMA 2: Open Foundation and Fine-Tuned Chat Models

Hugo Touvron* Louis Martin† Kevin Stone†

Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra  
Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen  
Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller  
Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou  
Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev  
Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich  
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra  
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi  
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang  
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang  
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic  
Sergey Edunov Thomas Scialom*

## GenAI,Meta

## Abstract

In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called  $\text{LLAMA} \, 2-\text{CHAT}$ , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-CHAT in order to enable the community to build on our work and contribute to the responsible development of LLMs.

## Contents

1 Introduction 3  
2 Pretraining 5

2.1 Pretraining Data 5  
2.2 Training Details 5  
2.3 LLAMA 2 Pretrained Model Evaluation 7

3 Fine-tuning 8

3.1 Supervised Fine-Tuning (SFT) 9  
3.2 Reinforcement Learning with Human Feedb</DOCUMENT>

RESP:
id: 1
content: # LLAMA 2: Open Foundation and Fine-Tuned Chat Models

Hugo Touvron* Louis Martin† Kevin Stone†

Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra  
Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen  
Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller  
Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou  
Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev  
Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich  
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra  
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi  
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang  
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang  
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic  
Sergey Edunov Thomas Scialom*

## GenAI, Meta

id: 2
content: ## Abstract

In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called LLAMA 2-CHAT, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-CHAT in order to enable the community to build on our work and contribute to the responsible development of LLMs.

id: 3
content: ## Contents

1 Introduction 3  
2 Pretraining 5
2.1 Pretraining Data 5  
2.2 Training Details 5  
2.3 LLAMA 2 Pretrained Model Evaluation 7

3 Fine-tuning 8
3.1 Supervised Fine-Tuning (SFT) 9  
3.2 Reinforcement Learning with Human Feedb

CONTEXT:
RESP:


CONTEXT:
RESP:


CONTEXT:
User:

Rules for chunking:
1. A chunk must represent a coherent idea, topic, or subtopic.
2. Do NOT split in the middle of a sentence or logical argument.
3. Prefer splitting at headings, subheadings, major paragraph breaks, or topic shifts.
4. Ideal chunk size: 300-600 words (or ~500-1200 tokens).
5. If a section is very long, split it into multiple coherent sub-chunks.
6. Preserve the original order of the text.

Output format:
id: 22
content: ...

id: 23
content: ...

Now chunk the following document:


<DOCUMENT>
# LLAMA 2: Open Foundation and Fine-Tuned Chat Models

Hugo Touvron* Louis Martin† Kevin Stone†

Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra  
Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen  
Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller  
Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou  
Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev  
Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich  
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra  
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi  
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang  
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang  
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic  
Sergey Edunov Thomas Scialom*

## GenAI,Meta

## Abstract

In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called  $\text{LLAMA} \, 2-\text{CHAT}$ , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-CHAT in order to enable the community to build on our work and contribute to the responsible development of LLMs.

## Contents

1 Introduction 3  
2 Pretraining 5

2.1 Pretraining Data 5  
2.2 Training Details 5  
2.3 LLAMA 2 Pretrained Model Evaluation 7

3 Fine-tuning 8

3.1 Supervised Fine-Tuning (SFT) 9  
3.2 Reinforcement Learning with Human Feedback (RLHF) 9  
3.3 System Message for Multi-Turn Consistency 16  
3.4 RLHF Results 17

4 Safety 20

4.1 Safety in Pretraining 20  
4.2 Safety Fine-Tuning 23  
4.3 Red Teaming 28  
4.4 Safety Evaluation of LLAMA 2-CHAT 29

5 Discussion 32

5.1 Learnings and Observations 32  
5.2 Limitations and Ethical Considerations 34  
5.3 Responsible Release Strategy 35

6 Related Work 35  
7 Conclusion 36  
A Appendix 46

A.1 Contributions 46  
A.2 Additional Details for Pretraining 47  
A.3 Additional Details for Fine-tuning 51  
A.4 Additional Details for Safety 58  
A.5 Data Annotation 72  
A.6 Dataset Contamination 75  
A.7 Model Card 77

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/efb40aa2874ba2af90fd13e297e03de151aa27d82aa3a701dc9cb341dc7f1f3b.jpg)  
Figure 1: Helpfulness human evaluation results for LLAMA 2-CHAT compared to other open-source and closed-source models. Human raters compared model generations on  $\sim 4k$  prompts consisting of</DOCUMENT>

RESP:
id: 1
content: # LLAMA 2: Open Foundation and Fine-Tuned Chat Models

Hugo Touvron* Louis Martin† Kevin Stone†

Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialom*

GenAI, Meta

id: 2
content: ## Abstract

In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called LLAMA 2-CHAT, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-CHAT in order to enable the community to build on our work and contribute to the responsible development of LLMs.

id: 3
content: ## Contents

1 Introduction 3
2 Pretraining 5
2.1 Pretraining Data 5
2.2 Training Details 5
2.3 LLAMA 2 Pretrained Model Evaluation 7
3 Fine-tuning 8
3.1 Supervised Fine-Tuning (SFT) 9
3.2 Reinforcement Learning with Human Feedback (RLHF) 9
3.3 System Message for Multi-Turn Consistency 16
3.4 RLHF Results 17
4 Safety 20
4.1 Safety in Pretraining 20
4.2 Safety Fine-Tuning 23
4.3 Red Teaming 28
4.4 Safety Evaluation of LLAMA 2-CHAT 29
5 Discussion 32
5.1 Learnings and Observations 32
5.2 Limitations and Ethical Considerations 34
5.3 Responsible Release Strategy 35
6 Related Work 35
7 Conclusion 36
A Appendix 46
A.1 Contributions 46
A.2 Additional Details for Pretraining 47
A.3 Additional Details for Fine-tuning 51
A.4 Additional Details for Safety 58
A.5 Data Annotation 72
A.6 Dataset Contamination 75
A.7 Model Card 77

id: 4
content: Figure 1: Helpfulness human evaluation results for LLAMA 2-CHAT compared to other open-source and closed-source models. Human raters compared model generations on ~4k prompts consisting of [content continues in original document]

CONTEXT:
RESP:


CONTEXT:
User:

Rules for chunking:
1. A chunk must represent a coherent idea, topic, or subtopic.
2. Do NOT split in the middle of a sentence or logical argument.
3. Prefer splitting at headings, subheadings, major paragraph breaks, or topic shifts.
4. Ideal chunk size: 300-600 words (or ~500-1200 tokens).
5. If a section is very long, split it into multiple coherent sub-chunks.
6. Preserve the original order of the text.

Output format:
id: 22
content: ...

id: 23
content: ...

Now chunk the following document:


<DOCUMENT>
# LLAMA 2: Open Foundation and Fine-Tuned Chat Models

Hugo Touvron* Louis Martin† Kevin Stone†

Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra  
Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen  
Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller  
Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou  
Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev  
Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich  
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra  
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi  
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang  
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang  
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic  
Sergey Edunov Thomas Scialom*

## GenAI,Meta

## Abstract

In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called  $\text{LLAMA} \, 2-\text{CHAT}$ , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-CHAT in order to enable the community to build on our work and contribute to the responsible development of LLMs.

## Contents

1 Introduction 3  
2 Pretraining 5

2.1 Pretraining Data 5  
2.2 Training Details 5  
2.3 LLAMA 2 Pretrained Model Evaluation 7

3 Fine-tuning 8

3.1 Supervised Fine-Tuning (SFT) 9  
3.2 Reinforcement Learning with Human Feedback (RLHF) 9  
3.3 System Message for Multi-Turn Consistency 16  
3.4 RLHF Results 17

4 Safety 20

4.1 Safety in Pretraining 20  
4.2 Safety Fine-Tuning 23  
4.3 Red Teaming 28  
4.4 Safety Evaluation of LLAMA 2-CHAT 29

5 Discussion 32

5.1 Learnings and Observations 32  
5.2 Limitations and Ethical Considerations 34  
5.3 Responsible Release Strategy 35

6 Related Work 35  
7 Conclusion 36  
A Appendix 46

A.1 Contributions 46  
A.2 Additional Details for Pretraining 47  
A.3 Additional Details for Fine-tuning 51  
A.4 Additional Details for Safety 58  
A.5 Data Annotation 72  
A.6 Dataset Contamination 75  
A.7 Model Card 77

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/efb40aa2874ba2af90fd13e297e03de151aa27d82aa3a701dc9cb341dc7f1f3b.jpg)  
Figure 1: Helpfulness human evaluation results for LLAMA 2-CHAT compared to other open-source and closed-source models. Human raters compared model generations on  $\sim 4k$  prompts consisting of both single and multi-turn prompts. The  $95\%$  confidence intervals for this evaluation are between  $1\%$  and  $2\%$ . More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent difficulty of comparing generations.

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/08f6863778b4206402cec247ce6410b9fcb0e70bd55573ad296b7ec92db20a46.jpg)  
Figure 2: Win-rate % for helpfulness and safety between commercial-licensed baselines and LAMA 2-CHAT, according to GPT-4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.

## 1 Introduction

Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.

The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al.</DOCUMENT>

RESP:
id: 1
content: # LLAMA 2: Open Foundation and Fine-Tuned Chat Models

Hugo Touvron* Louis Martin† Kevin Stone†

Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra  
Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen  
Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller  
Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou  
Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev  
Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich  
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra  
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi  
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang  
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang  
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic  
Sergey Edunov Thomas Scialom*

## GenAI, Meta

## Abstract

In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called LLAMA 2-CHAT, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-CHAT in order to enable the community to build on our work and contribute to the responsible development of LLMs.

id: 2
content: ## Contents

1 Introduction 3  
2 Pretraining 5
2.1 Pretraining Data 5  
2.2 Training Details 5  
2.3 LLAMA 2 Pretrained Model Evaluation 7

3 Fine-tuning 8
3.1 Supervised Fine-Tuning (SFT) 9  
3.2 Reinforcement Learning with Human Feedback (RLHF) 9  
3.3 System Message for Multi-Turn Consistency 16  
3.4 RLHF Results 17

4 Safety 20
4.1 Safety in Pretraining 20  
4.2 Safety Fine-Tuning 23  
4.3 Red Teaming 28  
4.4 Safety Evaluation of LLAMA 2-CHAT 29

5 Discussion 32
5.1 Learnings and Observations 32  
5.2 Limitations and Ethical Considerations 34  
5.3 Responsible Release Strategy 35

6 Related Work 35  
7 Conclusion 36  
A Appendix 46
A.1 Contributions 46  
A.2 Additional Details for Pretraining 47  
A.3 Additional Details for Fine-tuning 51  
A.4 Additional Details for Safety 58  
A.5 Data Annotation 72  
A.6 Dataset Contamination 75  
A.7 Model Card 77

id: 3
content: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/efb40aa2874ba2af90fd13e297e03de151aa27d82aa3a701dc9cb341dc7f1f3b.jpg)

Figure 1: Helpfulness human evaluation results for LLAMA 2-CHAT compared to other open-source and closed-source models. Human raters compared model generations on ~4k prompts consisting of both single and multi-turn prompts. The 95% confidence intervals for this evaluation are between 1% and 2%. More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent difficulty of comparing generations.

id: 4
content: ![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/08f6863778b4206402cec247ce6410b9fcb0e70bd55573ad296b7ec92db20a46.jpg)

Figure 2: Win-rate % for helpfulness and safety between commercial-licensed baselines and LLAMA 2-CHAT, according to GPT-4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.

id: 5
content: ## 1 Introduction

Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.

The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al.

CONTEXT:
RESP:


CONTEXT:
User:

Rules for chunking:
1. A chunk must represent a coherent idea, topic, or subtopic.
2. Do NOT split in the middle of a sentence or logical argument.
3. Prefer splitting at headings, subheadings, major paragraph breaks, or topic shifts.
4. Ideal chunk size: 300-600 words (or ~500-1200 tokens).
5. If a section is very long, split it into multiple coherent sub-chunks.
6. Preserve the original order of the text.

Output format:
id: 22
start char index: 345
end char index: 399

id: 23
start char index: 450
end char index: 550

Now chunk the following document:


<DOCUMENT>
# LLAMA 2: Open Foundation and Fine-Tuned Chat Models

Hugo Touvron* Louis Martin† Kevin Stone†

Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra  
Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen  
Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller  
Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou  
Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev  
Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich  
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra  
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi  
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang  
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang  
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic  
Sergey Edunov Thomas Scialom*

## GenAI,Meta

## Abstract

In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called  $\text{LLAMA} \, 2-\text{CHAT}$ , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-CHAT in order to enable the community to build on our work and contribute to the responsible development of LLMs.

## Contents

1 Introduction 3  
2 Pretraining 5

2.1 Pretraining Data 5  
2.2 Training Details 5  
2.3 LLAMA 2 Pretrained Model Evaluation 7

3 Fine-tuning 8

3.1 Supervised Fine-Tuning (SFT) 9  
3.2 Reinforcement Learning with Human Feedback (RLHF) 9  
3.3 System Message for Multi-Turn Consistency 16  
3.4 RLHF Results 17

4 Safety 20

4.1 Safety in Pretraining 20  
4.2 Safety Fine-Tuning 23  
4.3 Red Teaming 28  
4.4 Safety Evaluation of LLAMA 2-CHAT 29

5 Discussion 32

5.1 Learnings and Observations 32  
5.2 Limitations and Ethical Considerations 34  
5.3 Responsible Release Strategy 35

6 Related Work 35  
7 Conclusion 36  
A Appendix 46

A.1 Contributions 46  
A.2 Additional Details for Pretraining 47  
A.3 Additional Details for Fine-tuning 51  
A.4 Additional Details for Safety 58  
A.5 Data Annotation 72  
A.6 Dataset Contamination 75  
A.7 Model Card 77

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/efb40aa2874ba2af90fd13e297e03de151aa27d82aa3a701dc9cb341dc7f1f3b.jpg)  
Figure 1: Helpfulness human evaluation results for LLAMA 2-CHAT compared to other open-source and closed-source models. Human raters compared model generations on  $\sim 4k$  prompts consisting of both single and multi-turn prompts. The  $95\%$  confidence intervals for this evaluation are between  $1\%$  and  $2\%$ . More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent difficulty of comparing generations.

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-27/f26ef9f2-491c-4380-979e-5884a9f7fb5b/08f6863778b4206402cec247ce6410b9fcb0e70bd55573ad296b7ec92db20a46.jpg)  
Figure 2: Win-rate % for helpfulness and safety between commercial-licensed baselines and LAMA 2-CHAT, according to GPT-4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.

## 1 Introduction

Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.

The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al.</DOCUMENT>

RESP:
# Chunked Document

id: 1
start char index: 0
end char index: 892

id: 2
start char index: 893
end char index: 1547

id: 3
start char index: 1548
end char index: 2384

id: 4
start char index: 2385
end char index: 3102

CONTEXT:
